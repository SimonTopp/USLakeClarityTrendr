---
title: "3_eval_sites"
author: "Simon Topp"
date: "5/28/2019"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(tidyverse)
library(feather)
library(reticulate)
library(sf)
library(hydrolinks)
knitr::opts_chunk$set(echo = F)
```

## Pull out a couple sites per parameter for a general visual evaluation

```{r, eval = F}
## First, we need to pull out a couple lakes with lots of field data for model evaluation
#Pull in munged match up data
srMunged <- read_feather('out/srMunged.feather') %>%
  mutate(uniqueID = row_number())

#Load in landsat visible sites.
l.vis <- read_feather('../Aquasat/2_rsdata/out/unique_site_visible_inv.feather') %>%
  filter(med > 80)

# Load in lagos and wqp data and filter to landsat visible sites
wqp.lagos <- read_feather('../Aquasat/1_wqdata/out/wqp_lagos_unity.feather') %>%
  select(-c(doc, p_sand, tis)) %>%
  gather(chl_a, tss, secchi, key = 'parameter', value = 'value') %>%
  filter(!is.na(value)) %>%
  filter(SiteID %in% l.vis$SiteID)

############# For joining to metadata and pulling shapefiles###### Maybe delete
lake.data <- read_feather(paste0('out/dswe_NHDHLbySite.feather'))

# Pull sites with lots of in situ data but that won't take too many away from train/test data.
# Connect the counts to NHD data in munged parameters to make sure we're not duplicating lakes
counts <- wqp.lagos %>% 
  inner_join(srMunged %>% 
              select(SiteID, COMID) %>%
              distinct(), by = 'SiteID') %>%
  mutate(year = year(date_unity)) %>%
  filter(year >= 1984) %>%
  group_by(COMID, parameter, year) %>%
  summarize(count = n()) %>%
  filter(count > 3) %>% ## We only want years with at least 3 obs
  mutate(dumdum = 1) %>%
  group_by(COMID, parameter) %>%
  summarise(years.obs = sum(dumdum),
            total.obs = sum(count))

counts.filter <- counts %>%
  filter(years.obs > 18, ## Only lakes with at least 20 distinct years of data
         total.obs > 80 & total.obs < 500) %>% ##Take lakes with lots but not too much so we save train/test data
  inner_join(srMunged %>% 
              select(COMID, lat, long) %>%
              distinct(COMID, .keep_all = T), by = 'COMID')

counts.filter %>% group_by(parameter) %>% summarize(count = n())

# take a spatially stratified random sample of ~10 sites for each parameter
sampleSites <- function(i){
  sites <- i %>% 
  mutate(latR = cut_interval(lat, 10),
         longR = cut_interval(long, 10)) %>%
  group_by(latR, longR) %>%
  sample_n(1)
  #sample_frac(11/length(i$COMID))
  return(sites)
}

##Join sites to lake level data for modelling
set.seed(423)
sites.eval <- counts.filter %>%
  group_by(parameter) %>%
  nest() %>%
  mutate(sites = purrr::map(data, sampleSites)) %>%
  select(-data) %>%
  unnest(sites) %>%
  distinct(parameter, COMID, .keep_all = T) %>%
  group_by(parameter) %>%
  sample_n(8) %>%
  ungroup() %>%
  mutate(siteParam = paste0(COMID, parameter)) %>%
  left_join(srMunged %>% 
              mutate(period = ifelse(year <2009, 1, 2)) %>%
              distinct(COMID, period, .keep_all = T) %>%
              select(-c(parameter:value, NR:ndwi, uniqueID, month)),
            , by = 'COMID') %>%
  filter(!is.na(hylak_id),
         lake_area < 1000) #large lakes kill EE reducer

 
check <- check %>% group_by(parameter, COMID) %>% summarise(nsites =n(), nobs = sum(total.obs))
check %>% group_by(parameter) %>% summarize(count = n())
 
##pull and save the field observations for later
fieldObs <- wqp.lagos %>% 
  inner_join(srMunged %>% 
              select(SiteID, COMID) %>%
              distinct(), by = 'SiteID') %>%
  inner_join(sites.eval, by = c('COMID', 'parameter'))

## Take a look to see if we're happy
# Spatially
eval.sf <- st_as_sf(sites.eval,coords=c('long','lat'),crs=4326) %>%
    st_transform(.,2163)

usa = st_as_sf(map('usa',plot=F,fill=T)) %>%
    st_transform(.,2163) %>%
    st_buffer(.,0) 

ggplot() +
  geom_sf(data = usa, fill = 'grey90') + 
  geom_sf(data = eval.sf, aes(color = parameter))

fieldObs %>%
  mutate(lakeParam = paste0(COMID,parameter)) %>%
  ggplot(., aes(x = date_unity, y = value, shape = parameter)) + 
  geom_point() + 
  facet_wrap(~lakeParam, scales = 'free') + 
  theme(strip.background = element_blank(),
  strip.text.x = element_blank(),
  legend.position = 'top')

lakes.full <- lake.data %>% distinct(COMID, .keep_all = T) %>% filter(COMID %in% sites.eval$COMID)

#write_feather(sites.eval, 'out/evalSites.feather')
#write_feather(fieldObs, 'out/evalFieldObs.feather')

sites.eval <-read_feather('out/evalSites.feather')
fieldObs <- read_feather('out/evalFieldObs.feather')


rm(wqp.lagos) #Remove field data because it's real big

#Set cache for hydrolinks
hydrolinks::cache_set_dir(path = 'D:/hydroLinkstTmp')
eval.shp <- get_shape_by_id(sites.eval$COMID, 'waterbody', 'nhdplusv2')

# For now, just convert to a shapefile and upload manually, 
# Consider incorporating the upload into the code later on.
st_write(eval.shp, 'out/EvalSites/QAPull.shp')

use_condaenv('earthEngineGrabR')
```

```{r}
##repl_python basically starts a python bash window within your R chunk.  This can be used to actually interact with earth engine.
repl_python()

import time
import ee
import os
#import feather
ee.Initialize()

#Source necessary functions.
execfile('GEE_pull_functions_AM.py')

#Load in Pekel water occurance Layer and Landsat Collections.
PekelMask = False
pekel = ee.Image('JRC/GSW1_0/GlobalSurfaceWater')

l8 = ee.ImageCollection('LANDSAT/LC08/C01/T1_SR')
l7 = ee.ImageCollection('LANDSAT/LE07/C01/T1_SR')
l5 = ee.ImageCollection('LANDSAT/LT05/C01/T1_SR')

#Identify collection for use in sourced functions.
collection = 'SR'

#Standardize band names between the various collections and aggregate 
#them into one image collection

bn8 = ['B2','B3', 'B4', 'B5', 'B6','B7', 'pixel_qa']
bn57 = ['B1', 'B2', 'B3', 'B4', 'B5','B7', 'pixel_qa']
bns = ['Blue', 'Green', 'Red', 'Nir', 'Swir1', 'Swir2', 'qa']
  
ls5 = l5.select(bn57, bns)
ls7 = l7.select(bn57, bns)
ls8 = l8.select(bn8, bns)

ls = ee.ImageCollection(ls5.merge(ls7).merge(ls8))\
.filter(ee.Filter.lt('CLOUD_COVER', 50))

#Select the occurence layer in the pekel mask, which is just the 
#percentage of water occurence over a given pixel from 1985-2015.
#Set the percent occurance threshold and create a watermask from the result.
if PekelMask == True:
  threshold = 80
  water = pekel.select('occurrence').gt(threshold)
  water = water.updateMask(water)

lakes = ee.FeatureCollection('users/sntopp/aquaModel/ClarityTrendsEvalSites')

# Filter lakes that have already been downloaded
lakeID = lakes.aggregate_array('comid').getInfo()
dlDir = 'out/EvalSites/qaPull'
filesDown = os.listdir(dlDir)
filesDown = [int(i.replace(".csv", "")) for i in filesDown]

lakeID  = [i for i in lakeID if i not in filesDown]
                        
for x in range(0,len(lakeID)):
    lake = lakes.filter(ee.Filter.eq('comid', lakeID[x])).first()
    shell = ee.Feature(None)
    #FilterBounds for lake, update masks for water occurence, clouds, roads, etc.
    #Remove any images with clouds directly over the waterbody
    
    lsover = ls.filterBounds(lake.geometry())\
    .map(clipImage)
      
    
  ## Map over sites within specific path/row and pull reflectance values
    data = lsover.map(lakePull).filter(ee.Filter.lt('cScore', 25))

    dataOut = ee.batch.Export.table.toDrive(collection = data,\
                                              description = str(lakeID[x]),\
                                              folder = 'aquaModel_QAPull',\
                                              fileFormat = 'csv')
  #Check how many existing tasks are running and take a break if it's >15  
    maximum_no_of_tasks(20, 60)
  #Send next task.
    dataOut.start()
    print('done' + str(lakeID[x]))
    
print('done all')

## End the python bash.
exit
```

## Download the data to the local folder

```{r}
files <- drive_ls('aquaModel_QAPull')

for(i in c(1:length(files$name))){
  path = paste0('out/EvalSites/qaPull/',files$name[i])
  drive_download(file = as_id(files$id[i]), path = path)
}
```

