---
title: "4_LakeExport"
author: "Simon Topp"
date: "9/10/2018"
output: html_document
editor_options: 
  chunk_output_type: console
---
#Pull surface reflectance values for sample of nhd lakes, a random sample, and lakes over 10 sq km.

This script pulls down all the cloud free reflectance data available for a ranomd sample of 1000 nhd lakes.


```{r}
## Load in nhd lake data downlowded from, http://www.horizon-systems.com/NHDPlus/V2NationalData.php
nhdLakes <- st_read('D:/GIS_Data/NHD/nhdPlusV2_WaterBodiesFull.shp') %>%
  filter(FTYPE == 'LakePond' | FTYPE == 'Reservoir',
         AREASQKM >= .01, ##Make sure they're landsat visible
         AREASQKM < 4000)##Exclude the Great lakes


## Load in our region variable
region = st_read('in/NLA/NLA_Ecoregions/EcoRegsMerged.shp') %>%
  st_simplify(preserveTopology = T, dTolerance = 100) %>%
  st_transform(.,st_crs(nhdLakes))

plan(multiprocess)
centerJoin <- nhdLakes %>%
  st_centroid() %>% 
  split(., c(1:10)) %>%
  future_map_dfr(~st_join(.,region, join = st_nearest_feature), .progress = T)
plan(sequential)

nhd.join <- nhdLakes %>%
  inner_join(centerJoin %>% as_data_frame() %>% select(-geometry))

## Get counts to see how many matchups we have in each region
counts <- nhd.join %>%
  st_set_geometry(NULL) %>%
  group_by(region) %>%
  summarize(count = n())

# We'll do two pulls, first a random sample of 2,000 lakes per region and then a specific pull using just the NLA 2012 lakes because they're supposedly a statistically representative sample.  The rounding is formatting for writing to a shapefile.
set.seed(444743)
sample <- nhd.join %>%
  group_by(region) %>%
  sample_n(2000) %>%
  ungroup() %>%
  rename_at(vars(FDATE:LakeArea), tolower) %>%
  mutate_at(vars(shape_leng, shape_area, meandepth, lakearea), ~format(round(., 3), nsmall = 3)) %>%
  mutate(lakevolume = format(round(lakevolume, 2), nsmall = 2),
         COMID = factor(COMID))

#Make sure everything looks right
mapview(sample %>% st_centroid())

## OR, use the NLA 2012 sample because it's statistically representative
nlaSamp <- read.csv('in/NLA/nla2012_wide_siteinfo_08232016.txt', stringsAsFactors = F) %>% distinct(COMID2012, .keep_all = T) %>%
  filter(DSGN12 == 'Included')

## We loose 5 lakes that for some reason aren't in NHDPlusV2, not much to do about it.  The rounding is formatting for writing to a shapefile.
NLAsample <- nhd.join %>% filter(COMID %in% nlaSamp$COMID2012) %>%
  mutate(COMID = factor(COMID)) %>%
  rename_at(vars(FDATE:LakeArea), tolower) %>%
  mutate_at(vars(shape_leng, shape_area, meandepth, lakearea), ~format(round(., 3), nsmall = 3)) %>%
  mutate(lakevolume = format(round(lakevolume, 2), nsmall = 2),
         COMID = factor(COMID))


# For now, just convert to a shapefile and upload to EE manually, 
# Consider incorporating the upload into the code later on.
st_write(NLAsample, 'out/sampleLakes/NLA2012.shp')
st_write(sample, 'out/sampleLakes/EcoReg2000.shp')

##Make second centroid version of sample
sample <- st_read('out/sampleLakes/EcoReg2000.shp')
sample_cntr <- sample %>%
  st_transform(102003) %>%##Albers equal area is in meters
  st_point_on_surface() %>%
  st_buffer(120) %>%
  st_transform(4326) ##WGS84 for EE

st_write(sample_cntr, 'out/sampleLakes/EcoReg2000_cntr.shp')

## Make a centerpoint shapefile for the NHD lakes to be consistent with AquaSat
nla.ply <- st_read('out/sampleLakes/NLA2012.shp')

#If the centroid  is within the polygon, use that, otherwise use 
nla.pt <- nla.ply %>%
  st_transform(102003) %>% ##Albers equal area is in meters
  st_point_on_surface()%>%
  st_buffer(120) %>%
  st_transform(4326)

st_write(nla.pt, 'out/sampleLakes/NLA2012_cntr.shp')

# sample <- st_read('out/sampleLakes/EcoReg2000.shp')
# write_feather(sample %>%
#                 st_centroid(.) %>%
#                 st_transform(crs = 4326) %>%
#                 rowwise() %>%
#                 mutate(lat = geometry[2],
#                        long = geometry[1]) %>% as.data.frame() %>% select(-geometry), 'out/lakesEcoReg2000.feather')

lakes.join.out <- read_feather('out/lakesNLA2012.feather')


##Take a quick look to view the distribution of lakes.
lakes.sf <- st_read('out/sampleLakes/NLA2012.shp') %>%
  st_transform(., 2163) %>%
  st_point_on_surface()

mapView(lakes.sf)

## Pull lakes over 10 sq km to look at how trends vary with lake size.
over10 <- nhdLakes %>% filter(AREASQKM > 10) %>%
  #sample_n(500) %>%
  rename_at(vars(FDATE:LakeArea), tolower) %>%
  mutate_at(vars(shape_leng, shape_area, meandepth, lakearea), ~format(round(., 3), nsmall = 3)) %>%
  mutate(lakevolume = format(round(lakevolume, 2), nsmall = 2),
         COMID = factor(COMID)) %>%
  st_transform(102003) %>%##Albers equal area is in meters
  st_point_on_surface() %>%
  st_buffer(120) %>%
  st_transform(4326) %>%
  st_join(., region %>% st_transform(4326), st_nearest_feature) ##WGS84 for EE

st_write(over10, 'out/sampleLakes/Over10sqkm_cntr.shp')

write_feather(over10 %>%
                st_centroid(.) %>%
                st_transform(crs = 4326) %>%
                rowwise() %>%
                mutate(lat = geometry[2],
                       long = geometry[1]) %>% as.data.frame() %>% select(-geometry), 'out/Over10sqkm_cntr.feather')
```


## Finally, make one last subsample pulling the NLA 2007 lakes for validation 

```{r}
## Pull out the NHD 2007 lakes for validation
nla.2007 <- read.csv('in/NLA/nla2007_secchi_20091008.txt', stringsAsFactors = F) %>%
  select(SITE_ID, secchi = SECMEAN, date = DATE_SECCHI) %>%
  left_join(read.csv('in/NLA/nla2007_sampledlakeinformation_20091113.txt') %>%
              select(SITE_ID, COMID2007 = COM_ID, region = WSA_ECO9, 
                     lat = LAT_DD, long = LON_DD))

nla2007.lakes <- nla.2007 %>% st_as_sf(coords = c('long','lat'), crs = 4269) %>%
  st_join(nhdLakes %>% st_transform(4269)) %>%
  st_transform(102003) %>% ##Albers equal area is in meters
  st_buffer(120) %>%
  st_transform(4326) %>%# back to WGS84 for EarthEngine
  rename_at(vars(FDATE:LakeArea), tolower) %>%
  mutate_at(vars(shape_leng, shape_area, meandepth, lakearea),
            ~format(round(., 3), nsmall = 3)) %>%
  mutate(lakevolume = format(round(lakevolume, 2), nsmall = 2),
         COMID = factor(COMID))

st_write(nla2007.lakes, 'out/sampleLakes/NLA2007_cntr.shp')

lakes.join.out <- st_read('out/sampleLakes/NLA2007_cntr.shp') %>%
  st_set_geometry(NULL)

write_feather(lakes.join.out, 'out/lakesNLA2007.feather')

#use_python('/usr/local/bin/python2', required = T)
use_condaenv('earthEngineGrabR')
```

## Use python through the reticulate package to actually pull down the reflectance
values after manually uploading the above shapefiles to Earth Engine.

```{r}
#{python, engine.path ='/usr/local/bin/python2'}
##repl_python basically starts a python bash window within your R chunk.  This can be used to actually interact with earth engine.
repl_python()

import time
import ee
import os
#import feather
ee.Initialize()

#Source necessary functions.
execfile('GEE_pull_functions_AM.py')

#Load in Pekel water occurance Layer and Landsat Collections.
mask = 'dswe'
pekel = ee.Image('JRC/GSW1_0/GlobalSurfaceWater')

l8 = ee.ImageCollection('LANDSAT/LC08/C01/T1_SR')
l7 = ee.ImageCollection('LANDSAT/LE07/C01/T1_SR')
l5 = ee.ImageCollection('LANDSAT/LT05/C01/T1_SR')

#Identify collection for use in sourced functions.
collection = 'SR'

#Standardize band names between the various collections and aggregate 
#them into one image collection

bn8 = ['B2','B3', 'B4', 'B5', 'B6','B7', 'pixel_qa']
bn57 = ['B1', 'B2', 'B3', 'B4', 'B5','B7', 'pixel_qa']
bns = ['Blue', 'Green', 'Red', 'Nir', 'Swir1', 'Swir2', 'qa']
  
ls5 = l5.select(bn57, bns)
ls7 = l7.select(bn57, bns)
ls8 = l8.select(bn8, bns)

ls = ee.ImageCollection(ls5.merge(ls7).merge(ls8))\
.filter(ee.Filter.lt('CLOUD_COVER', 50))

#Select the occurence layer in the pekel mask, which is just the 
#percentage of water occurence over a given pixel from 1985-2015.
#Set the percent occurance threshold and create a watermask from the result.
# threshold = 80
# water = pekel.select('occurrence').gt(threshold)
# water = water.updateMask(water)


## Select the lake sample you want to pull
lakes = ee.FeatureCollection('users/sntopp/aquaModel/lakesEcoReg500') 

# Filter lakes that have already been downloaded
lakeID = lakes.aggregate_array('COMID').getInfo()
dlDir = 'lake_data/EcoReg500'
filesDown = os.listdir(dlDir)
filesDown = [i.replace(".csv", "") for i in filesDown]

lakeID  = [i for i in lakeID if i not in filesDown]
counter = 0                        
for x in range(0,len(lakeID)):
#for x in range(0,5):
  lake = lakes.filter(ee.Filter.eq('COMID', lakeID[x])).first()
  shell = ee.Feature(None)
  #FilterBounds for lake, update masks for water occurence, clouds, roads, etc.
  #Remove any images with clouds directly over the waterbody
  
  lsover = ls.filterBounds(lake.geometry())\
  .map(clipImage)
    
  
## Map over sites to pull the values
  data = lsover.map(lakePull)#.filter(ee.Filter.lt('cScore', 50))
  dataOut = ee.batch.Export.table.toDrive(collection = data, \
                                            description = str(lakeID[x]),\
                                            folder = 'aquaModel_lakesEcoReg500_looseFilt',\
                                            fileFormat = 'csv')
#Check how many existing tasks are running and take a break if it's >15  
  maximum_no_of_tasks(20, 60)
#Send next task.
  dataOut.start()
  counter = counter + 1
  print('done_' + str(counter))


#Make sure all Earth engine tasks are completed prior to moving on.  
maximum_no_of_tasks(1,300)
print('done')

## End the python bash.
#exit

```

# Download files to local directory

```{r}
files <- drive_ls('aquaModel_EcoReg2000_cntr')
files <- files %>% filter(!name %in% list.files('lake_data/EcoReg2000_cntr'))

for(i in c(1:nrow(files))){
  path <- paste0('lake_data/NLA2007_cntr/',files$name[i])
  drive_download(as_id(files$id[i]), path, overwrite = T)
}

```

