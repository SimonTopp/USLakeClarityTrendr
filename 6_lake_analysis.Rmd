---
title: "7.5_LakeModellingCorrelations"
author: "Simon Topp"
date: "4/17/2019"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup}
knitr::opts_chunk$set(echo = F)
knitr::opts_chunk$set(message = F)
knitr::opts_chunk$set(warning = F)
knitr::opts_chunk$set(results = 'hide')

```

# This document details the primary results of regionalized bootstrapping for remotely sensed lake water clarity timeseries.
## First, compare the predictions with NLA values as a final validation

```{r}

# ids <- list.files('lake_data/NLA2007_cntr') %>% strsplit(split = '.csv', fixed = T) %>% unlist()
# lakesDown <- list.files('lake_data/NLA2007_cntr', full.names = T)
# lake.join <- read_feather('out/NLA2007LakesFull.feather')
# ## Bad form, but you need to comment out part of the function for this because
# ## there's no l8 involved and it'll throw errors otherwise.
# plan(multiprocess(workers = availableCores()-4))
# 
# nla.2007.preds <- ids %>% future_map_dfr(~EvalPreds(id = .,paths = lakesDown, lakesUp = lake.join, log = log, model = model, features = features, lakeSamp = lakeSamp), .progress = T)
# 
# plan(sequential)
# 
# nla.2007.preds <- nla.2007.preds %>% mutate(region = factor(region,
#                          labels = c("Coastal Plain", "Northern Appalachians", "Northern Plains", "Southern Appalachians", "Southern Plains", "Temperate Plains", "Upper Midwest","Western Mountains","Xeric West")))
# write_feather(nla.2007.preds, 'out/nla2007_preds.feather')

nla.2007.preds <- read_feather('out/nla2007_preds.feather')

nla.2007 <- read.csv('in/NLA/nla2007_secchi_20091008.txt', stringsAsFactors = F) %>%
  select(SITE_ID, secchi = SECMEAN, date = DATE_SECCHI) %>%
  left_join(read.csv('in/NLA/nla2007_sampledlakeinformation_20091113.txt') %>%
              select(SITE_ID, COMID = COM_ID, region = WSA_ECO9, 
                     lat = LAT_DD, long = LON_DD))

nla.2012 <- read.csv('in/NLA/nla2012_secchi_08232016.txt', stringsAsFactors = F) %>%
  select(SITE_ID, secchi = SECCHI, date = DATE_COL) %>%
  left_join(read.csv('in/NLA/nla2012_wide_siteinfo_08232016.txt') %>%
              select(SITE_ID, COMID = COMID2012, region = AGGR_ECO9_2015, 
                     lat = LAT_DD83, long = LON_DD83))
nlaFull <- nla.2007 %>%
  mutate(nla.year = 2007) %>%
  bind_rows(nla.2012 %>% mutate(nla.year = 2012)) %>%
  mutate(date = mdy(date),
         year = year(date),
         month = month(date),
         region = factor(region, 
                         labels = c("Coastal Plain", "Northern Appalachians", "Northern Plains", "Southern Appalachians", "Southern Plains", "Temperate Plains", "Upper Midwest","Western Mountains","Xeric West"))) %>%
  group_by(SITE_ID, region, nla.year) %>%
  summarise(secchi = median(secchi, na.rm = T)) %>%
  group_by(region, nla.year) %>%
  rename(year = nla.year) %>%
  summarise(secchi.mean = mean(secchi, na.rm = T),
            secchi.median = median(secchi, na.rm = T),
            secchi.sd = sd(secchi, na.rm =T)) %>%
  mutate(source = 'NLA')


predComp <- nla.2007.preds %>% 
  filter(month %in% c(5:9)) %>%
  bind_rows(Preds.out %>%
  filter(month %in% c(5:9), year == 2012)) %>%
  mutate(source = 'Landsat') %>%
  na.omit() %>%
  group_by(year, region, source) %>%
  summarise(secchi.mean = mean(value, na.rm = T),
            secchi.median = median(value, na.rm = T),
            secchi.sd = sd(value, na.rm =T))  %>%
  bind_rows(nlaFull)
  
ggplot(predComp, aes(x = region, color = source)) +
  geom_point(aes(y = secchi.mean)) +
  #geom_point(aes(y = secchi.median)) +
  geom_errorbar(aes(ymin = secchi.mean - secchi.sd, ymax = secchi.mean + secchi.sd)) +
  scale_color_viridis_d(end = .7) +
  facet_wrap(~year) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(x = 'Region', y = 'Water Clarity (m)', title = 'Comparison of NLA to Remotely Sensed Predictions')

ggsave('figures/NLAComps.png', width = 6, height = 4, units = 'in')

check <- predComp %>% select(-secchi.median, -secchi.sd) %>% spread(source, secchi.mean)
rmse(check$NLA, check$Landsat)
bias(check$NLA, check$Landsat)
mape(check$NLA, check$Landsat)
mae(check$NLA, check$Landsat)

```

## Pull out median summer values first by lake and then region to examine non-monotic trends.

```{r}
## Take a quick look at mean trends in each HUC2

#How many lakes failed? The way the code is written we'll have 1 row of NA's for each lake that failed
colSums(is.na(Preds.out)) ##302 out of ~2.5k, not bad, in spot checking this is a landsat visible issue.

##Look at summer median values
Preds.out%>%
  left_join(lake.join %>% select(COMID, areasqkm)) %>%
  mutate(sizeClass = ifelse(areasqkm > 100,'big','small')) %>%
  na.omit() %>%
  filter(month %in% c(5:9)) %>%
  group_by(year, region) %>%
  summarise(value = mean(value)) %>%
  ggplot(., aes(x = year, y = value, color = region)) +
  geom_line() +
  geom_point() +
  labs(y = 'SDD (m)', title = 'SDD by HUC2 over time')

#Create dataframe of median summer values for each lake plus a mk-test stats
summ.meds <- Preds.out %>%
  filter(month %in% c(5:9),
         year < 2019) %>% ##Incomplete obs for 2019 so we won't use it.
  group_by(COMID, year, region) %>%
  summarise(secchi.med = median(value))

##Check how many years we have observations for each lake and only take lakes with > 10 years of obs. We loose ~500 lakes.
counts <- summ.meds %>%
  group_by(COMID) %>%
  summarise(count = n()) %>%
  #filter(count > 10) %>%
  left_join(lake.join)

##Quick figure illustrating proportion of lakes with data by year
summ.meds %>%
  filter(!is.na(value)) %>%
  group_by(COMID, year) %>%
  summarise(dataFrac = n()/2270) %>%
  ggplot(., aes(x = year, y = dataFrac)) + geom_col()

####  Create wide dataset 
summ.meds.wide <- summ.meds %>%
  filter(COMID %in% counts$COMID) %>%
  spread(year, secchi.med)

##Check regional counts to make sure they're reasonable
summ.meds.wide %>% group_by(region) %>% summarise(count = n())

###### Apply 1000 rounds of bootstrapping to yearly summer medians

## Map over the regions and pull out summary stats
bootstrapped.ts <- summ.meds.wide %>%
  group_by(region) %>%
  nest() %>%
  mutate(boot.means= purrr::map(data, ~boot(.,boot.med, R = 1000)),
         boots.summ = purrr::map(boot.means, boot.summary)) %>%
  select(-boot.means, -data) %>%
  unnest(boots.summ)

## Generate a figure showing the mean summer clarity and 'stability' (sd of bootstrap iterations) for each lake.
bootstrapped.ts %>%
  filter(year < 2019, year > 1984) %>%
  ggplot(., aes(x = year, y = mean)) +
  geom_errorbar(aes(ymin = mean - se, ymax = mean + se), color = 'grey60', alpha = .4) +
  geom_path(color = 'red') +
  theme_bw() +
  geom_point() +
  facet_wrap(~region, scales = 'free', ncol = 2) +
  labs(title = 'Mean and 90% confidence bound (stability)')

#####Look at MK stats for for each region.
summaryMK <- bootstrapped.ts %>%
  na.omit() %>%
  group_by(region) %>%
  arrange(year) %>%
  nest() %>%
  mutate(mk = purrr::map(data, ~sens.slope(.$mean)),
         sen.slope = purrr::map_dbl(mk, 'estimates'),
         sen.slope = sen.slope*100,
         #conf95 = purrr::map(mk, 'conf.int'),
         p.value = purrr::map_dbl(mk, 'p.value'),
         p.value = round(p.value, 5),
         sig = ifelse(p.value < .01, '***', ifelse(p.value < .05, '**', ifelse(p.value < 0.1, '*', NA)))) %>%
  select(-data, -mk)

## Finally plot up Man-Kendal results
summaryMK %>%
  left_join(region) %>%
  rowwise() %>%
  mutate(coords.x = st_point_on_surface(geometry)[1],
    coords.y = st_point_on_surface(geometry)[2]) %>%
  ungroup() %>%
  ggplot() +
    geom_sf(aes(fill = sen.slope)) +
    geom_text(aes(x = coords.x, y = coords.y, label = sig), size = 4, color = 'red') +
    theme(axis.text.x = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks = element_blank(),
          rect = element_blank(),
          panel.grid.major = element_line(color = 'transparent'),
          legend.position = 'bottom') +
    scale_color_manual(values = 'red', breaks = 'yes') +
    scale_fill_viridis_c(option = 'cividis') +
    labs(fill = 'Slope (cm/year)', title = 'MannKendall Slopes') +
    guides(color = guide_legend(label = F))

write_feather(bootstrapped.ts, paste0('out/TS_Preds/',lakeSamp,'_bootstrapped.feather'))
write_feather(summaryMK, paste0('out/TS_Preds/',lakeSamp,'_summaryMK.feather'))
```


## Compare remotely sensed and in situ observations

```{r}
fieldMean <- read_feather('out/RegionalsInSituMeans.feather')

meanFull <- fieldMean %>% 
  mutate(source = 'In.Situ', se = NA, bias = NA) %>%
  bind_rows(read_feather('out/TS_Preds/NLA2012_cntr_bootstrapped.feather') %>%
              mutate(source = 'NLA')) %>%
  bind_rows(read_feather('out/TS_Preds/EcoReg2000_bootstrapped.feather') %>%
              mutate(source = 'Random')) %>% 
  bind_rows(read_feather('out/TS_Preds/Over10_bootstrapped.feather') %>%
              mutate(source = 'Large.Lakes'))

nla.rand.cor <- meanFull %>% select(year,region, mean, source) %>% spread(source, mean) %>%
  group_by(region) %>%
  nest() %>%
  mutate(NLA.Random.Corr = purrr::map(data, ~cor.test(.$NLA, .$Random)),
         cor = purrr::map_dbl(NLA.Random.Corr, 'estimate'),
         p.value = purrr::map_dbl(NLA.Random.Corr, 'p.value'))
nla.situ.cor <- meanFull %>% select(year,region, mean, source) %>% spread(source, mean) %>%
  group_by(region) %>%
  nest() %>%
  mutate(NLA.Random.Corr = purrr::map(data, ~cor.test(.$NLA, .$In.Situ)),
         cor = purrr::map_dbl(NLA.Random.Corr, 'estimate'),
         p.value = purrr::map_dbl(NLA.Random.Corr, 'p.value'),
         sig = ifelse(p.value < .01, '***', ifelse(p.value < .05, '**', ifelse(p.value < 0.1, '*', NA))))


colors = cividis(9, begin = .1, end = .9, direction = -1)

p1 <- meanFull %>%
  filter(year < 2019, year > 1984) %>%
  filter(source != 'In.Situ', source != 'Large.Lakes') %>%
  ggplot(., aes(x = year, y = mean, group = source, color = source)) +
  geom_errorbar(aes(ymin = mean - se, ymax = mean + se), color = 'grey60', alpha = .8) +
  geom_line() +
  theme_bw() +
  #geom_point() +
  scale_color_viridis_d(end = .5) +
  facet_wrap(~region, scales = 'free_y', ncol = 2) +
  labs(y = 'Mean Summer Clarity (m)', x = 'Year') +
  theme(strip.text = element_text(colour = 'white'),
        legend.position = c(.75, .05),
        legend.direction = 'horizontal',
        axis.text.x = element_text(hjust = 1)) # c(0,0) bottom left, c(1,1) top-right.)

g <- ggplot_gtable(ggplot_build(p1))
strip_both <- which(grepl('strip-', g$layout$name))

k <- 1
for (i in strip_both[c(1,3:10)]) {
j <- which(grepl('rect', g$grobs[[i]]$grobs[[1]]$childrenOrder))
g$grobs[[i]]$grobs[[1]]$children[[j]]$gp$fill <- colors[k]
k <- k+1
}
grid::grid.draw(g)

ggsave('figures/NLA_TS_Comp.png', width = 5, height = 4.5, plot = g)

FieldMK <- fieldMean %>%
  group_by(region) %>%
  arrange(year) %>%
  nest() %>%
  mutate(mk = purrr::map(data, ~sens.slope(.$mean)),
         sen.slope = purrr::map_dbl(mk, 'estimates'),
         sen.slope = sen.slope*100,
         p.value = purrr::map_dbl(mk, 'p.value'),
         p.value = round(p.value, 5),
         sig = ifelse(p.value < .01, '***', ifelse(p.value < .05, '**', ifelse(p.value < 0.1, '*', NA)))) %>%
  select(-data, -mk)


## Compare NLA and Large Random Sample
lakes <- read_feather('out/NLA2012LakesFull.feather') %>%
  mutate(sample = 'NLA.Sample') %>%
  bind_rows(read_feather('out/EcoReg2000LakesFull.feather') %>% mutate(sample = 'Random.Sample')) %>%
  st_as_sf(coords = c('long', 'lat'), crs = 4326)

ggplot(lakes, aes(x = areasqkm, fill = sample)) + geom_density(alpha = .3) + scale_x_log10()

mk.full <- FieldMK %>%
  mutate(source = 'In.Situ') %>%
  bind_rows(read_feather('out/TS_Preds/NLA2012_cntr_summaryMK.feather') %>%
              mutate(source = 'NLA.Sample')) %>%
  bind_rows(read_feather('out/TS_Preds/EcoReg2000_summaryMK.feather') %>%
              mutate(source = 'Random.Sample')) %>%
    bind_rows(read_feather('out/TS_Preds/Over10_summaryMK.feather') %>%
              mutate(source = 'Large.Lakes.Sample')) %>%
  mutate(source = factor(source, levels = c('NLA.Sample', 'Random.Sample', 'Large.Lakes.Sample', 'In.Situ')))


trendDif <- mk.full %>%
  select(region, sen.slope, source) %>%
  spread(source, sen.slope) %>%
  mutate(NLA.minus.Random = NLA.Sample - Random.Sample,
         NLA.minus.Large.Lakes = NLA.Sample - Large.Lakes.Sample,
         NLA.minus.In.Situ = NLA.Sample - In.Situ) %>%
  gather(NLA.minus.Random, NLA.minus.Large.Lakes, NLA.minus.In.Situ, key = 'Group', value = 'Slope.Difference') %>%
  mutate(sig = NA, p.value = NA, source = 'Difference') %>%
  left_join(region %>% st_simplify(dTolerance = 500))


p2 <- ggplot(region %>% st_simplify(dTolerance = 500)) + geom_sf(aes(fill = region)) + 
  geom_sf(data = lakes, size = .2, color = 'black', alpha = .4) + 
  theme(axis.text.x = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks = element_blank(),
          rect = element_blank(),
          panel.grid.major = element_line(color = 'transparent'),
          plot.margin = margin(-2,0,-2, 0, 'cm'),
        strip.text = element_text(face = 'bold'),
        legend.position = 'none') +
  scale_fill_viridis_d(option = 'cividis', name = "Region", end = .9, begin = .1) + 
  facet_wrap(~sample)

ggsave('figures/SamplePointMaps.png', height = 3, width = 6, plot = p2)

p2 <- mk.full %>%
  filter(source != 'Large.Lakes.Sample') %>%
  mutate(sen.slope = ifelse(sen.slope > 1, 1, sen.slope),
         sen.slope = ifelse(sen.slope < -.5, -.5, sen.slope)) %>%
  left_join(region %>% st_simplify(dTolerance = 500)) %>%
  rowwise() %>%
  mutate(coords.x = st_point_on_surface(geometry)[1],
    coords.y = st_point_on_surface(geometry)[2]) %>%
  ungroup() %>%
  ggplot() +
    geom_sf(aes(fill = sen.slope), color = 'grey30') +
    geom_text(aes(x = coords.x, y = coords.y, label = sig), size = 4, color = 'black') +
    theme(axis.text.x = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks = element_blank(),
          rect = element_blank(),
          panel.grid.major = element_line(color = 'transparent'),
          legend.position = 'top',
          plot.margin = margin(-1,0,-1, 0, 'cm')) +
    scale_fill_viridis_c(option = 'plasma', begin = .2, breaks = c(-.5,0,.5,1), labels = c('< -0.5', '0', '0.5',' > 1.0')) +
    #scale_fill_gradient2(low='#F56217', high='#0B486B') +
    labs(fill = 'Slope (cm/year)', x = '', y = '') +
    guides(color = guide_legend(label = F)) +
  facet_wrap(~source) +
  theme(strip.text = element_text(face="bold"))

p3 <- trendDif %>% filter(Group == 'NLA.minus.Random' | Group == 'NLA.minus.In.Situ') %>%
  mutate(Slope.Difference = ifelse(Slope.Difference < -.5, -.5, Slope.Difference),
         Slope.Difference = ifelse(Slope.Difference > .5, .5, Slope.Difference)) %>%
  ggplot() +
  geom_sf(aes(fill = Slope.Difference), color = 'grey30') +
  theme(axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks = element_blank(),
        rect = element_blank(),
        plot.margin = margin(-1,0,0, 0, 'cm'),
        panel.grid.major = element_line(color = 'transparent'),
        legend.position = 'right') +
  scale_fill_gradient2(low='#a50026',mid='#ffffff', high='#313695', 
                       midpoint = 0, breaks = c(-.5, -.25,0,.25,.5), labels = c('< -0.5','','0','','> 0.5')) +
  facet_wrap(~Group)+
  theme(strip.text = element_text(face="bold"))

g <- grid.arrange(p2, p3, nrow = 2)

ggsave('figures/SlopeComps.png',plot = g, width = 6.5, height = 5, units = 'in')

g <- grid.arrange(p1,p2, p3, layout_matrix = rbind(c(1,1,1),c(2,2,3)))


p1 <- mk.full %>%
  filter(source != 'In.Situ', source != 'Large.Lakes.Sample') %>%
  left_join(region %>% st_simplify(dTolerance = 500)) %>%
  rowwise() %>%
  mutate(coords.x = st_point_on_surface(geometry)[1],
    coords.y = st_point_on_surface(geometry)[2]) %>%
  ungroup() %>%
  ggplot() +
    geom_sf(aes(fill = sen.slope), color = 'grey30') +
    geom_text(aes(x = coords.x, y = coords.y, label = sig), size = 4, color = 'black') +
    theme(axis.text.x = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks = element_blank(),
          rect = element_blank(),
          panel.grid.major = element_line(color = 'transparent'),
          legend.position = c(.5, -.33),
          legend.direction = 'horizontal',) +
    scale_fill_viridis_c(option = 'plasma', begin = .2) +
    #scale_fill_gradient2(low='#F56217', high='#0B486B') +
    labs(fill = 'Slope (cm/year)', x = '', y = '') +
    guides(color = guide_legend(label = F)) +
  facet_wrap(~source) +
  theme(strip.text = element_text(face="bold"))

p2 <- ggplot(trendDif %>% filter(Group == 'NLA.minus.Random')) +
  geom_sf(aes(fill = Slope.Difference), color = 'grey30') +
  theme(axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks = element_blank(),
        rect = element_blank(),
        plot.margin = margin(-.07, unit = 'cm'),
        panel.grid.major = element_line(color = 'transparent'),
        legend.position = c(.3, -.2),
        legend.direction = 'horizontal',
        strip.text = element_text(face="bold")) +
  scale_fill_gradient2(low='#a50026',mid='#ffffff', high='#313695', 
                       midpoint = 0) +
  facet_wrap(~Group)

g <- grid.arrange(p1,p2, ncol = 2, widths = c(2,1))

ggsave('figures/SampleComps.png', plot = g, width = 6.5, height = 3)

```

## Individual lake analysis

```{r}
summ.meds.full <- read_feather(paste0('out/TS_Preds/NLA2012_cntr_',iteration,'.feather')) %>% mutate(sample = 'NLA') %>%
  bind_rows(read_feather(paste0('out/TS_Preds/Over10_',iteration,'.feather')) %>% mutate(sample = 'Over10')) %>%
  bind_rows(read_feather(paste0('out/TS_Preds/EcoReg2000_cntr_',iteration,'.feather')) %>%
              mutate(sample = 'Random')) %>%
  filter(month %in% c(5:9),
  year < 2019) %>% ##Incomplete obs for 2019 so we won't use it.
  group_by(COMID, year, region) %>%
  summarise(secchi.med = median(value))

lake.join <- read_feather('out/NLA2012LakesFull.feather') %>%
  bind_rows(read_feather('out/Over10LakesFull.feather')) %>%
  bind_rows(read_feather('out/EcoReg2000LakesFull.feather')) %>%
  distinct()

#For individual lake analysis use stricter filter for total years with observations
counts <- summ.meds.full %>%
  group_by(COMID, region) %>%
  summarise(count = n()) %>%
  filter(count > 25)

summ.meds.full <- summ.meds.full %>% filter(COMID %in% counts$COMID)

summ.meds.full %>% mutate(Period = ifelse(year < 1999, '1984-1989', ifelse(year > 2012, '2013-2018', NA))) %>% filter(!is.na(Period)) %>% mutate(Period = factor(Period)) %>%
  left_join(lake.join) %>%
  mutate(Size.Group = cut(areasqkm, breaks = c(0,1,10,100,2000), labels = c('<1','1-10', '10-100', '>100'))) %>%
  ggplot(aes(x = secchi.med)) + geom_density(aes(fill = Size.Group), alpha = .4) + scale_fill_viridis_d(end = .8) + theme_bw() +
  facet_wrap(~Period, nrow = 2) + 
  ggtitle('Distribution of median summer clarity\nby lake size')

##Calculate mann-Kendall statistics for each lake.
mk <- summ.meds.full %>% 
  group_by(COMID, region) %>%
  arrange(year) %>%
  nest() %>%
  mutate(mk = purrr::map(data, ~sens.slope(.$secchi.med)),
         sen.slope = purrr::map_dbl(mk, 'estimates'),
         sen.slope = sen.slope*100,
         p.value = purrr::map_dbl(mk, 'p.value'),
         sig = ifelse(p.value < .1, T, F)) %>%
  select(COMID, region, sen.slope, p.value, sig) %>%
  left_join(lake.join) %>%
  mutate(Size.Group = cut(areasqkm, breaks = c(0,10,100,2000), labels = c('<10', '10-100', '>100')))

mk.ave <- mk %>% group_by(Size.Group) %>% summarise(mean =mean(sen.slope), median = median(sen.slope), sd = sd(sen.slope), lower90 = mean - sd, upper90 = mean + sd)

ggplot(mk, aes(x = sen.slope)) + geom_density(aes(fill = Size.Group), alpha = .4) + scale_fill_viridis_d(end = .7) + theme_bw() + labs(title = 'Distribution of Lake Trends by Size Class', x = 'Thiel-Sen Slope (cm/yr)', y = 'Density', color = 'Size Group\n(sq km)', fill = 'Size Group\n(sq km)') + geom_vline(data = mk.ave, aes(xintercept = median, color = Size.Group)) + scale_color_viridis_d(end = .9) + coord_cartesian(xlim = c(-5,5))

ggsave('figures/TrendbySizeClass.png', width = 6, height = 3, units = 'in')

wilcox.test(mk$sen.slope[mk$Size.Group == '<10'], mk$sen.slope[mk$Size.Group == '10-100'], conf.int = T)
wilcox.test(mk$sen.slope[mk$Size.Group == '10-100'], mk$sen.slope[mk$Size.Group == '>100'], conf.int = T)
wilcox.test(mk$sen.slope[mk$Size.Group == '<10'], mk$sen.slope[mk$Size.Group == '>100'], conf.int = T)
wilcox.test(mk$sen.slope[mk$Size.Group == '>100'], conf.int = T)
wilcox.test(mk$sen.slope[mk$Size.Group == '10-100'],conf.int = T)
wilcox.test(mk$sen.slope[mk$Size.Group == '<10'],conf.int = T)



mk %>% group_by(Size.Group) %>% summarise(mean =mean(sen.slope), median = median(sen.slope))

ggplot(mk, aes(x = areasqkm, y = sen.slope)) + geom_point() + scale_x_log10()

ggplot(mk, aes(sizeQuant, sen.slope)) + geom_violin(trim = F) + theme_bw() + ggtitle('Distribution of Lake Trends by Region') + stat_summary(fun.data=mean_sdl, mult=1, geom="pointrange", color="red")#+ geom_boxplot(width=0.5)


mk %>% filter(!is.na(lat), areasqkm > 10) %>%st_as_sf(., coords = c('long','lat'), crs = 4326) %>% st_transform(st_crs(region)) %>%
  ggplot(.) +   geom_sf(data = region) + geom_sf(aes(color = sen.slope, alpha = sig)) +
  scale_color_gradient2(low='#a50026',mid='#ffffff', high='#313695', 
                       midpoint = 0) +
  scale_alpha_discrete(breaks = c(T,F), limits = c(1,.3), labels = c('True', 'False'))

#Look at the distribution of clarity change across in-network and out-of-network lakes
ggplot(mk, aes(x = sen.slope)) + geom_density(alpha = .4, aes(fill = factor(inStreamCat))) 


##Quick look at size distributions from LAGOS
lagos.lakes <- read_csv('in/LAGOS/Lagos_lakeInfo.txt')
lagos.secchi <- readRDS('in/LAGOS/Sol_data.rds') %>%
  select(secchi, lagoslakeid, sampledate) %>% na.omit() %>%
  left_join(lagos.lakes) %>%
  mutate(areasqkm = lake_area_ha/100)

ggplot(lagos.secchi, aes(x = lake_area_ha)) + geom_density() + scale_x_log10()

mean(lagos.secchi$areasqkm)
median(lagos.secchi$areasqkm)
```


##Quick look at how many obs we're averaging over for each month

```{r, include = F, eval = F}
## Average lake observations per region/month
monthCounts <- Preds.out %>%
  filter(year < 2019,
         month %in% c(5,6,7,8,9)) %>%
  group_by(COMID, year, region) %>%
  dplyr::summarize(count = n()) %>%
  group_by(year, region) %>%
  dplyr:: summarise(count.obs = sum(count),
            count.lakes = n())

#Make a 3 panel figure for how many lakes we observe each month and the total observations

quantiles <- tibble(quant.label = factor(seq(10,90,10)), 
                    quants = quantile(monthCounts$count.obs, c(seq(.1,.9,.1))))

p1 <- monthCounts %>%
  ggplot(., aes(x = count.obs)) +
  geom_histogram(binwidth = 150, color = 'black') +
  geom_vline(data = quantiles, aes(color = quant.label, xintercept = quants), size = 1, alpha = .6) +
  #scale_x_continuous(breaks = pretty_breaks(n = 10)) +
  scale_color_viridis_d()+
  labs(x = 'Regional Observations Per Year', color = '10% Quantiles') +
  theme_bw() +
  theme(legend.position = 'none')

quantiles <- tibble(quant.label = factor(seq(10,90,10)), 
                    quants = quantile(monthCounts$count.lakes, c(seq(.1,.9,.1))))
p2 <- monthCounts %>%
  ggplot(., aes(x = count.lakes)) +
  geom_histogram(binwidth = 20, color = 'black', fill = 'grey80') +
  geom_vline(data = quantiles, aes(color = quant.label, xintercept = quants), size = 1, alpha = .6) +
  scale_color_viridis_d()+
  #scale_x_continuous(breaks = pretty_breaks(n = 10)) +
  labs(x = 'Regional Lake Count Per Year', color = '10% Quantiles') +
  theme_bw() +
  theme(legend.position = 'bottom')

g <- grid.arrange(p1,p2, nrow = 2, heights = c(.3,.35))

p3 <- Preds.out %>%
  filter(month %in% c(5:9)) %>%
  group_by(region, month, year) %>%
  summarise(count.obs = n()) %>%
  group_by(region, month) %>%
  summarise(count.obs = mean(count.obs)) %>%
  ggplot(., aes(x = region, y = count.obs, fill = factor(month))) +
  geom_col(position = 'dodge') +
  scale_fill_viridis_d()+
  labs(x = 'Region', y = 'Average Observations per Month', fill = 'Month') +
  theme_bw() +
  theme(legend.position = 'bottom',
        axis.text.x = element_text(angle = 30, hjust = 1, vjust = 1))

g <- grid.arrange(p1,p2, p3, nrow = 3, heights = c(.3,.35,.35))

ggsave(plot = g, 'figures/MonthlyObs3Panel_IceFree.png', width = 6.5, height = 9, units = 'in')

## Take a look at individual lake ts and regional medians
ggplot() + 
  geom_line(data = Preds.out %>% na.omit(.), aes(group = COMID, x = ymd_hms(date), y = value, color = 'Individual Lake'), alpha = .2) + 
  geom_line(data = summ.meds %>% mutate(date = ymd(paste0(year,'-06-15'))), aes(x = date, y = secchi.med, color = 'Watershed Median')) +
  labs(x = 'Date', y = 'Water Clarity (m)', title = 'Individual Lake Timeseries', color = 'Time Series', fill = '') +
  scale_color_manual(values = c('black', 'red')) +
  facet_wrap(~region, scales = 'free') +
  theme_bw() +
  theme(legend.position = 'bottom')

#ggsave(paste0('figures/',mod,'_FullTSSE.png'), width =  7, height = 6, units = 'in')
```

## Look Percent Change over time and space

```{r}
###
p1 <- bootstrapped.ts %>% group_by(region) %>% mutate(p.dif = ((mean/lag(mean))-1)*100)%>%
  ungroup() %>%
  ggplot(.,aes(x = region, y = p.dif)) + geom_violin(aes(fill = region)) + 
  geom_boxplot(width=0.1, color = 'grey80', fill = 'grey60') + 
  scale_fill_viridis_d(option = 'cividis', begin = .1, end = .9) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 25, hjust = 1),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        #axis.ticks.x = element_blank(),
        legend.position = 'none') 
  #labs(y = 'Yearly Percent Clarity Change')

p.dif.meds <- bootstrapped.ts %>% group_by(region) %>% mutate(p.dif = ((mean/lag(mean))-1)*100) %>% group_by(year) %>% summarise(median = median(p.dif, na.rm = T)) %>%
  mutate(trend = ifelse(median > 0, 'Increase','Decrease')) %>% na.omit()

year.change.counts <-bootstrapped.ts %>% group_by(region) %>% 
  mutate(p.dif = ((mean/lag(mean))-1)*100, 
         increase = ifelse(p.dif > 0 , 1,0), 
         decrease = ifelse(p.dif < 0, 1, 0)) %>% group_by(year) %>% summarise(increase = sum(increase), decrease = sum(decrease))

p.dif.quants <- bootstrapped.ts %>% group_by(region) %>% mutate(p.dif = ((mean/lag(mean))-1)*100, p.dif = round(p.dif, 2)) %>% 
  summarise(quant25 = quantile(p.dif, .25, na.rm = T),
            quant50 = quantile(p.dif, .5, na.rm = T),
            quant75 = quantile(p.dif, .75, na.rm = T),
            range25.75 = quant75-quant25)

p2 <- bootstrapped.ts %>% group_by(region) %>% mutate(p.dif = ((mean/lag(mean))-1)*100) %>%
  ungroup() %>% filter(year != 1984) %>% ggplot(., aes(x = factor(year), y = p.dif)) + 
  geom_violin(fill = 'grey80') +
  geom_point(data = p.dif.meds, aes(x = factor(year), y = median, color = trend)) + 
  scale_color_manual(values = c('brown', 'blue'), name = 'Median Clarity Change') +
  labs(x = 'Year') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, vjust = .5),
        axis.title.y = element_blank(),
        legend.position = c(.68,.9),
        legend.direction = 'horizontal',
        legend.background = element_blank())
  
g <- grid.arrange(p1, p2, nrow = 2, left = 'Yearly Percent Change')

ggsave(plot = g, 'figures/AnnualPercentChange.png', width = 6.5, height = 4, units = 'in')
```


## Take a look at correlations with NADP and PRISM data

```{r, include = F, eval = F}
#Unzip and join all the prism data
## Pull in PRISM data (originally downloaded through the PRISM ftp)
##Everything is zipped, which is super annoying
dezip <- function(path, var){
  files <- list.files(path, pattern = '*.zip', full.names = T)
  purrr::map(files, unzip, exdir = 'D:/PRISM_MeanTemp/unzipped')
  }

#Unzip files to local paths
paths <- list.dirs('D:/PRISM_Precip', recursive = F)
purrr::map(paths, dezip)

paths <- list.dirs('D:/PRISM_MeanTemp', recursive = F)
purrr::map(paths, dezip)

## Reproject and simplify regions to match PRISM raster files for the spatial join
regionNAD83 <- region %>% 
  st_simplify(., dTolerance = 15000) %>%
  st_transform(., 4269)

ggplot(regionNAD83) + geom_sf(aes(fill = region))

geo <- regionNAD83
raster.path <- pathTemps[1]
## Make function for pulling out summary mean values per HUC_2
getMeans <- function(raster.path, geo){
  geo <- as(geo, "Spatial")
  image <- raster(raster.path)
  name <- image@data@names
  yearmonth <- str_split(name, pattern = '_')[[1]][5]
  year = substr(yearmonth,1,4)
  month = substr(yearmonth,5,6)
  means <- extract(image, geo, fun = mean, na.rm = T, sp = T)
  means <- means@data
  colnames(means)[3] <- 'value'
  means$year = year
  means$month = month
  return(means)
}

library(raster)
## It's pretty slow, so lets try to put it all in parrallel
pathTemps <- list.files('D:/PRISM_MeanTemp/unzipped', pattern = '[0-9]{6}_bil.bil$', full.names = T)
pathPrecip <- list.files('D:/PRISM_Precip/unzipped', pattern = '[0-9]{6}_bil.bil$', full.names = T)
plan(multiprocess(workers = availableCores()-4))

prismTemps <- pathTemps %>%
  future_map_dfr(getMeans, geo = regionNAD83, .progress = T) %>%
  rename_all(tolower) %>%
  mutate_at(vars(year,month), as.numeric) %>%
  filter(month %in% c(5:9)) %>%  
  group_by(region, year) %>%
  summarise(value = mean(value, na.rm = T))

prismPrecip <- pathPrecip %>%
  future_map_dfr(getMeans, geo = regionNAD83, .progress = T) %>%
  rename_all(tolower) %>%
  mutate_at(vars(year,month), as.numeric) %>%
  filter(month %in% c(5:9)) %>%  
  group_by(region, year) %>%
  summarise(value = mean(value, na.rm = T))

plan(sequential)

write_feather(prismTemps, paste0('out/PRISMTemp_',area,'.feather'))
write_feather(prismPrecip, paste0('out/PRISMPrecip_',area,'.feather'))

#raster and furrr randomly don't play well together sometimes.  If the above fails go the slow way.

prismTemps <- pathTemps %>%
  map_dfr(getMeans, geo = regionNAD83)

prismPrecip <- pathPrecip %>%
  map_dfr(getMeans, geo = regionNAD83)

# Detach raster package because it doesn't play nice with dplyr
detach("package:raster", unload = TRUE)
```


```{r, include = F, eval = F}
prismTemps <- read_feather('out/PRISMTemp_eco.feather')
  
prismPrecip<- prismPrecip %>% ungroup() %>%mutate(region = as.character(region), region = ifelse(region == 'Temperate Planes', 'Temperate Plains', region), region = factor(region))
write_feather(prismPrecip, 'out/PRISMPrecip_eco.feather')  

prismPrecip <- read_feather('out/PRISMPrecip_eco.feather')

## Pull in the NADP Data
nadp.sites <- read.csv('../aquaModel/in/NADP/siteList.csv') %>%
  filter(!is.na(lat)) %>%
  st_as_sf(coords = c('long','lat'), crs = 4326) %>%
  st_transform(.,st_crs(region)) %>%
  st_join(region)
  
#ggplot(nadp.sites) +  geom_sf(data =usa) + geom_sf()

nadp.yearly <- read.csv('../aquaModel/in/NADP/NTN-All-m.csv') %>%
  left_join(nadp.sites, by = 'SiteID') %>%
  rename(year = yr) %>%
  select(-c(Criteria1:Criteria3, fullChemLab, svol, ppt, daysSample, elev)) %>%
  filter(month %in% c(5:9)) %>%
  group_by(region, year) %>%
  dplyr::summarize_if(is.numeric, mean, na.rm = T)
  

##Teleconnection Indexes
tc <- read.csv('../aquaModel/in/IsoPdo.csv') %>%
  mutate(year = as.numeric(substr(YrMonth, 1,4)),
         month = as.numeric(substr(YrMonth, 5,6))) %>%
  filter(month %in% c(5:9)) %>%
  group_by(year) %>%
  summarise_at(vars(PDO, PNA, SOI, NAO), mean)


## Make a master dataset of all the stl, nadp, and prism data.
bootstrapped.full <- bootstrapped.ts %>%
  left_join(nadp.yearly) %>%
  left_join(prismTemps %>% rename(Temp = value)) %>%
  left_join(prismPrecip %>% rename(Precip = value)) %>%
  left_join(tc)

write_feather(bootstrapped.full, paste0('out/bootstrappedFull_',iteration,'.feather'))
```


```{r}
bootstrapped.full <- read_feather(paste0('out/bootstrappedFull_',iteration,'.feather'))

# Just take a look at the data a little  
# p <- stl.full %>%
#   select(region, date, trend_mean, pH, raw_mean) %>%
#   gather(trend_mean:raw_mean, key = 'metric', value = 'value') %>%
#   ggplot(., aes(x = date, y = value, color = metric)) + 
#   geom_line(alpha = .3) +
#   geom_smooth(se = F, span = .3) +
#   facet_wrap(~region, scales = 'free')
# 
# ggplotly(p)

#Look at overall correlation
corr.t <- bootstrapped.full %>%
  na.omit() %>%
  select(region, mean, NH4, NO3, pH, PNA, NAO, SO4, PDO, Precip, Temp) %>%
  gather(NH4:Temp, key = 'Metric', value = 'Conc') %>%
  group_by(region, Metric) %>%
  nest() %>%
  mutate(cors = purrr::map(data, ~cor.test(x = .x$mean, y = .x$Conc)),
         cor = purrr::map(cors, 'estimate'),
         cor = purrr::map_dbl(cor, 'cor'),
         p.value = purrr::map_dbl(cors, 'p.value')) %>%
  select(-c(data, cors))%>%
  mutate_at(vars(cor:p.value), round, digits =3) %>%
  mutate(sig = ifelse(p.value < .05, 'yes', NA),
         Metric = factor(Metric, levels = c('Temp', 'Precip', 'NO3', 'NH4', 'SO4', 'pH', 'PNA', 'PDO', 'NAO')),
         signal = 'Trend') %>%
  left_join(region)

corr.t %>%
  filter(Metric %in% c('SO4', 'Temp', 'Precip', 'PDO')) %>%
  st_as_sf() %>%
  st_simplify(., dTolerance = 1000) %>%
  ggplot(.) +  geom_sf(aes(fill = cor, color = sig)) + 
  facet_wrap(~Metric) + 
  scale_fill_gradient2(low = 'red', high = 'blue', mid = 'grey', midpoint = 0) +
  scale_color_manual(values = 'black', na.translate = F) +
  theme_bw() +
  theme(axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks = element_blank(),
        #rect = element_blank(),
        legend.position = 'bottom') +
  labs(fill = 'Correlation\nCoefficient', color = 'P.Value < 0.05',
       title = 'Potential Correlates with Overall Trend') +
  guides(color = guide_legend(label = F))

ggsave(paste0('figures/',mod,'RegionalCorrelates.png'), height = 7, width = 7.5, units = 'in')
```



## Cluster analysis
```{r, include = F, eval = F}
library(dtwclust)

##Check how many lakes we have lots of obs for
counts <- Preds.out %>% 
  group_by(COMID, year, month) %>%
  summarise(count = n()) %>%
  group_by(COMID) %>%
  summarise(count = n())


t.matrix <- huc2.full %>% select(region, date, trend_mean) %>% filter(!is.na(trend_mean)) %>% spread(key = date, value = trend_mean) %>% as.data.frame() %>% set_rownames(.,levels(huc2.full$region)) %>% select(-region) %>% t() %>% na.omit() %>% t()

pc_t <- tsclust(t.matrix, k = 3,
distance = "dtw_basic", centroid = "mean",
seed = 94L)
cvi(pc_t, type = 'internal')

s.matrix <- huc2.full %>% select(region, date, seasonal_mean) %>% filter(!is.na(seasonal_mean)) %>% spread(key = date, value = seasonal_mean) %>% as.data.frame() %>% set_rownames(.,levels(huc2.full$region)) %>% select(-region) %>% t() %>% na.omit() %>% t()

pc_s <- tsclust(s.matrix, k = 3,
distance = "dtw_basic", centroid = "mean",
seed = 94L)
cvi(pc_s, type = 'internal')

r.matrix <- huc2.full %>% select(region, date, raw_mean) %>% filter(!is.na(raw_mean)) %>% spread(key = date, value = raw_mean) %>% as.data.frame() %>% set_rownames(.,levels(huc2.full$region)) %>% select(-region) %>% t() %>% na.omit() %>% t()

pc_r <- tsclust(r.matrix, k = 3,
distance = "dtw_basic", centroid = "mean",
seed = 94L)
cvi(pc_r, type = 'internal')

clusters <- tibble(region = row.names(t.matrix), tClust = pc_t@cluster, sClust = pc_s@cluster, rClust = pc_r@cluster)

check <- clusters %>%
  gather(tClust:rClust, key = 'Component', value = 'Cluster') %>%
  left_join(region)


p <- ggplot(check %>% mutate(Cluster = factor(Cluster))) +
  geom_sf(aes(fill = Cluster)) + 
  theme_bw() +
  theme(axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks = element_blank(),
        #rect = element_blank(),
        legend.position = 'bottom') +
  facet_wrap(~Component)

#ggsave(p, file = 'figures/ClusterAnalysis.png', width = 7, height = 4, units = 'in')


huc2.full <- huc2.full %>% left_join(clusters)

p1 <- huc2.full %>%
  ggplot(aes(x = date, y = trend_mean, color = region, group = region)) + 
  geom_ribbon(aes(ymin = trend_mean - trend_sd, ymax = trend_mean + trend_sd), fill = 'grey90', color = 'grey90') +
  geom_line() + 
  scale_color_viridis_d() +
  theme_bw() +
  theme(legend.position = 'none') +
  facet_wrap(~tClust, nrow = 2, scales = 'free')

p1

grid.arrange(grobs = list(clustTrends, hucSamples), nrow = 2)

## Code for comparing n clusters
# pc_k <- tsclust(s.matrix, k = 3:6,
# distance = "dtw_basic", centroid = "mean",
# seed = 94L)
# names(pc_k) <- paste0("k_", 3L:6L)
# sapply(pc_k, cvi, type = "internal")
```


# Quick look at seasonality

```{r}
seasonal <- Preds.out %>%
  mutate(Period = ifelse(year < 1996, '1984-1995',
                           ifelse(year >1995 & year < 2007, '1996-2006', '2007-2018'))) %>%
  filter(month %in% c(5:9)) %>%
  group_by(region, month, Period) %>%
  summarise(value = median(value))

ggplot(seasonal, aes(x = month, y = value, color = Period)) + 
  geom_smooth(se = F, span = .4, size = .7) + 
  theme_bw() + 
  theme(legend.position = 'bottom') +
  labs(x = 'Month', y = 'Water Clarity (m)', title = 'Regional Lake Water Clarity Seasonality') +
  facet_wrap(~region, scales = 'free_y') + 
  scale_color_viridis_d(direction = -1, end = .8)

ggsave('figures/AGU_seasonality.png', width = 6, height = 5, units = 'in')

seasonal <- Preds.out %>%
  filter(month %in% c(4:10)) %>%
  group_by(region, month, year) %>%
  summarise(value = mean(value)) 

seasonal %>%
  ggplot(., aes(x = month, y = value, color = year, group = year))+ 
  #geom_line() +
  geom_smooth(span = .3, se = F) + 
  scale_color_viridis_c() + 
  facet_wrap(~region, scales = 'free') 

##GIFS round 2
#Select area
subset = 'Southern Plains'

##Make TS image
## Single watersheds
seasonal %>%
  ggplot(., aes(x = month, y = value, color = , size = Signal, group = Signal)) +
  geom_path() +
  scale_color_manual(values = c('grey60', 'red')) +
  scale_size_manual(values = c(.7,1)) +
  theme_bw() +
  labs(y = 'Water Clarity (m)', x = 'Date', title = subset) +
  theme(plot.caption = element_text(hjust = .7, size = 7.5))

ggsave(paste0('figures/',subset,'.png'), width = 4, height = 2.5, units = 'in')

#Make Static seasonal image

Preds.out %>% 
  filter(region == subset, 
         year == 1985,
         !is.na(seasonal_mean)) %>%
  ggplot(., aes(x = month, y = seasonal_mean)) +
    geom_smooth(se = F, span = .4, aes(color = 'Seasonal Pattern')) +
    #geom_ribbon(aes(ymin = seasonal_mean -2*seasonal_sd, ymax = seasonal_mean+2*seasonal_sd), alpha = .3)+
    geom_point(aes(x = month.min, y = min(seasonal_mean)-max(seasonal_sd), color= 'Minimum Clarity'), size = 2) +
    geom_point(aes(x = month.max, y = min(seasonal_mean)-max(seasonal_sd), color = 'Maximum Clarity'), size = 2) +
    scale_color_manual(values = c('blue', 'brown', 'black')) + 
    scale_x_continuous(breaks = c(1:12), labels = c(1:12)) +
    #theme(legend.position = 'none') +
    labs(x = 'Month', y = 'Seasonal Clarity Signal (m)', color = '')+
    theme(legend.position = 'bottom') +
    ggtitle(subset, 'Year: 1985')

#ggsave(paste0('figures/',region,'_static.png'), width = 3.5, height = 3.5, units = 'in')

p <- seasonal %>%
  filter(region == subset) %>% 
  ggplot(., aes(x = month, y = value)) +
    geom_smooth(se = F, span = .4, aes(color = 'Seasonal Pattern')) +
    #geom_ribbon(aes(ymin = seasonal_mean -2*seasonal_sd, ymax = seasonal_mean+2*seasonal_sd), alpha = .3)+
    #geom_point(aes(x = month.max, y = min(seasonal_mean)-max(seasonal_sd), color = 'Minimum Clarity'), size = 2) +
    #geom_point(aes(x = month.min, y = min(seasonal_mean)-max(seasonal_sd), color= 'Maximum Clarity'), size = 2) +
    #scale_color_manual(values = c('blue', 'brown', 'black')) + 
    #scale_x_continuous(breaks = c(1:12), labels = c(1:12)) +
    #theme(legend.position = 'none') +
    labs(x = 'Month', y = 'Seasonal Clarity Signal (m)', color = '')+
    theme(legend.position = 'bottom') +
  transition_states(year, transition_length = .5, state_length = 1) +
  ggtitle(subset, 'Year: {closest_state}')


pgif <- animate(p, width = 3.5, height = 3.5, units = 'in', res = 250)

grid.arrange(p1, rasterGrob(pgif), bottom = 'Trends and seasonality of lake water clarity in the Tennessee Region HUC 2 watershed')

anim_save(paste0('figures/',subset,'.gif'))

#Combine into 1 image.
a_mgif <- image_read('figures/Tennessee Region.png')
b_mgif <- image_read(pgif)

new_gif <- image_append(c(a_mgif, b_mgif[1]))
for(i in 1:35){
  combined <- image_append(c(a_mgif, b_mgif[i]))
  new_gif <- c(new_gif, combined)
}

image_write_gif(new_gif,'figures/test.gif')

image_read(new_gif)

colors <- tibble(colors = viridis(18))



```

