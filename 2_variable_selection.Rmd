---
title: "__FFS and Spatial CV"
author: "Simon Topp"
date: "8/27/2019"
output: html_document
editor_options: 
  chunk_output_type: console
---
# This script takes the munged optical and landscape data and runs it through various feature selection algorithms to determine the inputs for the final models.

```{r setup, eval = F}
library(googledrive)
library(tidyverse)
library(feather)
library(viridis)
library(knitr)
library(sf)
library(rgdal)
library(maps)
library(magrittr)
library(mlbench)
library(caret)
library(randomForest)
library(doParallel)
library(furrr)
library(lubridate)
library(groupdata2)
library(CAST)
library(mapview)
library(onehot)
library(Metrics)
library(kableExtra)
library(ggpmisc)
```

#Attempt at FFS Using xgboost and spatial cross validation.

```{r}
srMunged <- read_feather('out/srMunged.feather') %>%
  filter(parameter == 'secchi') %>%
  left_join(read_feather('out/lakeCatMunged.feather') %>% mutate(COMID = as.character(COMID)))


##Join to the NLA Ecoregions
region <- st_read('in/NLA/NLA_Ecoregions/EcoRegsMerged.shp') %>%
  st_simplify(., dTolerance = 1500) %>%
  st_transform(., 4326)


sr.sf <- srMunged %>%
  st_as_sf(coords = c('long', 'lat'), crs = 4326) %>%
  st_join(region, join = st_nearest_feature) %>%
  filter(!is.na(region))

## Check the final counts
counts <- sr.sf %>%
  group_by(parameter)%>%
  summarise(count = n())

## Select parameters to go into model
paste0(names(sr.sf), collapse = ', ')

features <- "green, nir, red, NR, GR, GN, brightness, CatAreaSqKm, inStreamCat, Al2O3Cat, CaOCat, Fe2O3Cat, K2OCat, MgOCat, Na2OCat, P2O5Cat, SCat, SiO2Cat, NCat, HydrlCondCat, CompStrgthCat, KffactCat, PctCarbResidCat, PctNonCarbResidCat, PctAlkIntruVolCat, PctSilicicCat, PctExtruVolCat, PctColluvSedCat, PctGlacTilClayCat, PctGlacTilLoamCat, PctGlacTilCrsCat, PctGlacLakeCrsCat, PctGlacLakeFineCat, PctHydricCat, PctEolCrsCat, PctEolFineCat, PctSalLakeCat, PctAlluvCoastCat, PctCoastCrsCat, PctWaterCat, Precip8110Cat, Tmax8110Cat, Tmean8110Cat, Tmin8110Cat, RunoffCat, ClayCat, SandCat, OmCat, PermCat, RckdepCat, WtDepCat, BFICat, ElevCat, AOD, brightness"

# features <- "blue, green, nir, red, sat, swir1, swir2, NR, BR, GR, SR, BG, BN, BS, GS, GN, fai, ndvi, ndwi, AOD, fui.hue"

features <- str_split(features, pattern = ', ')[[1]]

groups = c('secchi')
i <- 'secchi'

## Loop over the variables iteratively conducting the recurrent feature elimination, could do this with a mapped function,
## but this is a little easier.

## After some examination of the data, consider removing temporally explicit variables that can potentially lead to step changes in resulting model

#for (i in groups){
  
#Test with SDD < 15
  df <- sr.sf %>%
    filter(parameter == i,
           !is.na(region),
           value <= 15) %>%
    mutate(value = log(value)) %>%
    rowwise() %>%
    mutate(long = geometry[1], lat = geometry[2]) %>%
    ungroup() %>%
    select(-geometry)

  
  #If only using xgboost turn missing values into NA
  df <- df %>% na_if(-9999)
  #RF can't accept NA values, so drop our missing numbers our missing number to NAs
  #df[df == -9999] <- NA
  #df <- df %>%na.omit()
  ##Pull holdout data
  set.seed(340987)
  holdOut <- df %>%
    group_by(region) %>%
    sample_frac(.2)
  
  df <- df %>% filter(!uniqueID %in% holdOut$uniqueID)
  

  #One hot encoding for xgboost
  oneHot <- onehot(df %>% select(one_of(features)))
  df.encoded <- predict(oneHot, df %>% select(one_of(features))) %>% as.data.frame(.) 
  
  # Split up cross validation folds based on stratified sampling
  df <- df %>% mutate(SpaceTime = paste0(region, sat),
                      g1 = cut_number(lat, 3),
                      date = ymd(date),
                      julian = as.numeric(julian.Date(date))) %>%
              group_by(g1) %>%
              mutate(g2 = cut_number(long, 3)) %>%
              ungroup() %>%
              mutate(SpaceCluster = paste0(g1,g2),
                     timeCluster = cut_number(julian, 9))
  
  folds <- CreateSpacetimeFolds(df, spacevar = 'SpaceCluster', timevar = 'timeCluster', k= 3, seed = 34985)
  
  control <- trainControl(method="cv", savePredictions = T, 
                          returnResamp = 'final', index = folds$index, 
                          indexOut = folds$indexOut)
  
  #ffs.tuneGrid <- expand.grid(mtry = 2)
  ffs.tuneGrid <- expand.grid(nrounds = 500, max_depth = 4, eta = .1, 
                              gamma = 1, min_child_weight = 1, colsample_bytree = .6,
                              subsample = .6)
  
  # Takes awhile, so set it up all to run in parrallel
  cl <- makePSOCKcluster(availableCores() - 4)
  registerDoParallel(cl)
  
  ffs <- ffs(df.encoded, df$value, method = 'xgbTree', metric = 'RMSE', 
             tuneGrid = ffs.tuneGrid, trControl = control, verbose = T)

  stopCluster(cl)
  
  
  
  if(i == first(groups)){
    ffsResults <- ffs$perf_all %>%
      mutate(parameter = i)
    
    ffsVariables <- tibble(var = list(ffs$selectedvars),
                          parameter = i)
  }else{
    ffsResults <- ffsResults %>%
      bind_rows(ffs$perf_all %>% 
                  mutate(parameter = i))
    ffsVariables <- ffsVariables %>%
      bind_rows(tibble(var = list(ffs$selectedvars),
                          parameter = i))
  }
  print(paste0('done ',i))
#  }


ffsVariables <- ffsVariables %>%
  mutate(n_vars = map_int(var, length)) %>%
  rowwise() %>%
  # Collapse the returned lists into a single character vector, we'll expand this back out later.
  mutate(var = paste(unlist(var), sep= '', collapse = ', '))

# Save the results
write_feather(ffsResults, 'out/ffsResultsFull.feather')
write_feather(ffsVariables,'out/ffsVariablesFull.feather')

ffsResults %>%
  group_by(nvar, parameter) %>%
  summarise(RMSE = median(RMSE),
            SE = median(SE)) %>%
  ggplot(.) + geom_line(aes(x = nvar, y = RMSE)) +
  geom_errorbar(aes(x = nvar, ymin = RMSE - SE, ymax = RMSE + SE), color = 'red') +
  facet_wrap(~parameter, scales = 'free')

#ggsave('figures/rfeRMSE.png', device = 'png', width = 6, height = 4, units = 'in')
```

### XGBoost with Default Hyperparameters

```{r baseline model, message=FALSE}

inputs <- c(str_split(ffsVariables$var, ', ')[[1]])#, 'sat=7','sat=8')
inputs <- str_split("blue, green, nir, red, sat=5, sat=7, sat=8, swir1, swir2, NR, BR, GR, SR, BG, BN, BS, GS, GN, fai, ndvi, ndwi, AOD, fui.hue", ', ')[[1]] 
# We'll set up a tuning grid with default values, define a `trainControl()`
# function (which in this case disables printing training log to console
# plus disables resampling as we want to use all training data to fit the
# model), and finally fit the model with `caret::train()`:

grid_default <- expand.grid(
  nrounds = 100,
  max_depth = 4,
  eta = 0.3,
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

#Set up a cluster to run everything in parrallel
cl <- makePSOCKcluster(availableCores()/2)
registerDoParallel(cl)

train_control <- caret::trainControl(method="cv", savePredictions = T, 
                          returnResamp = 'final', index = folds$index, 
                          indexOut = folds$indexOut)

xgb_base <- caret::train(
  x = df.encoded %>% select(inputs),
  y = df$value,
  trControl = train_control,
  tuneGrid = grid_default,
  method = "xgbTree",
  verbose = TRUE
)
```

# Grid Search for Hyperparameters

```{r First tune, warning=FALSE, fig.height=7}
# Next, we build tuning grid for `caret` to explore between different
# hyperparameter values. To get started we're using some suggestions from
# [here](https://www.slideshare.net/OwenZhang2/tips-for-data-science-competitions/14).
# 
# ## Step 1: Number of Iterations and the Learning Rate
# 
# We going to start the tuning "the bigger knobs" by setting up the maximum
# number of trees:

# note to start nrounds from 200, as smaller learning rates result in errors so
# big with lower starting points that they'll mess the scales


# To get reasonable running time while testing hyperparameter combinations
# with `caret` we don't want to go over `r nrounds`. Then, we want to find a good enough learning rate for this number of trees,
# as for lower learning rates `r nrounds` iterations might not be enough.
# 
# Next, as the maximum tree depth is also depending on the number of iterations
# and the learning rate, we want to experiment with it at this point to narrow
# down the possible hyperparameters. We'll also create a helper function to 
# create the visualizations with `ggplot2`, called `tuneplot()`:

nrounds <- 1000

tune_grid <- expand.grid(
  nrounds = seq(from = 100, to = nrounds, by = 100),
  eta = c(0.05, 0.1, 0.3),
  max_depth = c(2, 3, 4),
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

tune_control <- caret::trainControl(
  method = "cv", # cross-validation
  index = folds$index,
  indexOut = folds$indexOut,# with n folds 
  verboseIter = FALSE, # no training log
  allowParallel = TRUE # FALSE for reproducible results 
)

xgb_tune <- caret::train(
  x = df.encoded %>% select(inputs),
  y = df$value,
  trControl = tune_control,
  tuneGrid = tune_grid,
  method = "xgbTree",
  verbose = TRUE
)

# helper function for the plots
tuneplot <- function(x, probs = .90) {
  ggplot(x) +
    coord_cartesian(ylim = c(quantile(x$results$RMSE, probs = probs), min(x$results$RMSE))) +
    theme_bw()
}

tuneplot(xgb_tune)
xgb_tune$bestTune
```


```{r second tune, warning=FALSE, fig.height=5}

# With `r nrounds` iterations a learning rate of `r xgb_tune$bestTune$eta`
# seems to be a good starting point. The model currently has a cross-validated
# RMSE of `r round(min(xgb_tune$results$RMSE), digits = 5)`. Also to be noted 
# that with `max_depth = 2` and smaller learning rates the forest seems not yet
# stabile.
# 
# ## Step 2: Maximum Depth and Minimum Child Weight
# 
# After fixing the learning rate to `r xgb_tune$bestTune$eta` and we'll also
# set maximum depth to `r xgb_tune$bestTune$max_depth` +-1 (or +2 if 
# `max_depth == 2`) to experiment a bit around the suggested best tune in
# previous step. Then, well fix maximum depth and minimum child weight:


tune_grid2 <- expand.grid(
  nrounds = seq(from = 50, to = nrounds, by = 50),
  eta = xgb_tune$bestTune$eta,
  max_depth = ifelse(xgb_tune$bestTune$max_depth == 2,
    c(xgb_tune$bestTune$max_depth:4),
    xgb_tune$bestTune$max_depth - 1:xgb_tune$bestTune$max_depth + 1),
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = c(1, 2, 3),
  subsample = 1
)

xgb_tune2 <- caret::train(
  x = df.encoded %>% select(inputs),
  y = df$value,
  trControl = tune_control,
  tuneGrid = tune_grid2,
  method = "xgbTree",
  verbose = TRUE
)

tuneplot(xgb_tune2)
xgb_tune2$bestTune
```


```{r third tune, warning=FALSE, fig.height=5}
# The second tune results RMSE of `r round(min(xgb_tune2$results$RMSE), digits=5)`,
# `r glue("{round((min(xgb_tune$results$RMSE)/min(xgb_tune2$results$RMSE) - 1)*100, digits=2)}%")`
# over the previous step.
# 
# ## Step 3: Column and Row Sampling
# Based on this, we can fix minimum child weight to `r xgb_tune2$bestTune$min_child_weight`
# and maximum depth to `r xgb_tune2$bestTune$max_depth`. Next, we'll try
# different values for row and column sampling:

tune_grid3 <- expand.grid(
  nrounds = seq(from = 50, to = nrounds, by = 50),
  eta = xgb_tune$bestTune$eta,
  max_depth = xgb_tune2$bestTune$max_depth,
  gamma = 0,
  colsample_bytree = c(0.4, 0.6, 0.8, 1.0),
  min_child_weight = xgb_tune2$bestTune$min_child_weight,
  subsample = c(0.5, 0.75, 1.0)
)

xgb_tune3 <- caret::train(
  x = df.encoded %>% select(inputs),
  y = df$value,
  trControl = tune_control,
  tuneGrid = tune_grid3,
  method = "xgbTree",
  verbose = TRUE
)

tuneplot(xgb_tune3, probs = .95)
xgb_tune3$bestTune
```

```{r fourth tune, warning=FALSE, fig.height=5}

# Based on the above, we can set column sampling to `r xgb_tune3$bestTune$colsample_bytree`
# and row sampling to `r xgb_tune3$bestTune$subsample`. RMSE at this point is `r round(min(xgb_tune3$results$RMSE), digits=5)`, which is
# `r glue("{round((min(xgb_tune2$results$RMSE)/min(xgb_tune3$results$RMSE)-1) * 100, digits=2)}%")`
# better than RMSE in step 2.
# 
# ## Step 4: Gamma
# Next, we again pick the best values from previous step, and now will see 
# whether changing the gamma has any effect on the model fit:
# 


tune_grid4 <- expand.grid(
  nrounds = seq(from = 50, to = nrounds, by = 50),
  eta = xgb_tune$bestTune$eta,
  max_depth = xgb_tune2$bestTune$max_depth,
  gamma = c(0, 0.05, 0.1, 0.5, 0.7, 0.9, 1.0),
  colsample_bytree = xgb_tune3$bestTune$colsample_bytree,
  min_child_weight = xgb_tune2$bestTune$min_child_weight,
  subsample = xgb_tune3$bestTune$subsample
)

xgb_tune4 <- caret::train(
  x = df.encoded %>% select(inputs),
  y = df$value,
  trControl = tune_control,
  tuneGrid = tune_grid4,
  method = "xgbTree",
  verbose = TRUE
)

tuneplot(xgb_tune4)
xgb_tune4$bestTune
```


```{r fifth tune, warning=FALSE}
# After the inspection, we can set the gamma to `r xgb_tune4$bestTune$gamma`. 
# RMSE of this model is `r round(min(xgb_tune4$results$RMSE), digits=5)`
# `r if(xgb_tune4$bestTune$gamma!=0){glue("which is {round((min(xgb_tune3$results$RMSE)/min(xgb_tune4$results$RMSE)-1)*100, digits=2)}% improvement.")}else{"."}`
# 
# ## Step 5: Reducing the Learning Rate
# 
# Now, we have tuned the hyperparameters and can start reducing the learning
# rate to get to the final model:

tune_grid5 <- expand.grid(
  nrounds = seq(from = 100, to = 1000, by = 100),
  eta = c(0.1, .15, .2), #c(0.01, 0.015, 0.025, 0.05, 0.1),
  max_depth = xgb_tune2$bestTune$max_depth,
  gamma = xgb_tune4$bestTune$gamma,
  colsample_bytree = xgb_tune3$bestTune$colsample_bytree,
  min_child_weight = xgb_tune2$bestTune$min_child_weight,
  subsample = xgb_tune3$bestTune$subsample
)

xgb_tune5 <- caret::train(
  x = df.encoded %>% select(inputs),
  y = df$value,
  trControl = tune_control,
  tuneGrid = tune_grid5,
  method = "xgbTree",
  verbose = TRUE
)

tuneplot(xgb_tune5)
xgb_tune5$bestTune
```


```{r final model, comment=NA}

# These will be the hyperparameters for the final model. RMSE now sets to 
# `r round(min(xgb_tune5$results$RMSE), digits=5)`, resulting in improvement of
# `r glue("{round((min(xgb_tune4$results$RMSE)/min(xgb_tune5$results$RMSE)-1)*100, digits=2)}%")`
# over the step 4, which in total is
# `r glue("{round((min(xgb_tune$results$RMSE)/min(xgb_tune5$results$RMSE)-1)*100, digits=2)}%")`
# better than the first tune.
# 
# ## Fitting the Model
# 
# Now that we have determined the parameters we want to use, we will use the
# training data (excluding the hold-out set which we will soon use to measure
# the model performance) without resampling to fit the model:


(final_grid <- expand.grid(
  nrounds = xgb_tune5$bestTune$nrounds,
  eta = xgb_tune5$bestTune$eta,
  max_depth = xgb_tune5$bestTune$max_depth,
  gamma = xgb_tune5$bestTune$gamma,
  colsample_bytree = xgb_tune5$bestTune$colsample_bytree,
  min_child_weight = xgb_tune5$bestTune$min_child_weight,
  subsample = xgb_tune5$bestTune$subsample
))

(model <- caret::train(
  x = df.encoded %>% select(inputs),
  y = df$value,
  trControl = train_control,
  tuneGrid = final_grid,
  method = "xgbTree",
  verbose = TRUE
))


stopCluster(cl)


```


```{r hold-out RMSE}

# Note, that this model is not trained with whole training set yet which we will
# still do before submitting the predictions.
# 
# # Evaluating the Model Performance
# 
# Now we will evaluate the model performance by:
# 
#  - Testing the performance of the two baseline models and the final model with the hold-out data
#  - Submitting predictions to Kaggle from default hyperparameter XGBoost plus the tuned one 
#  - Testing whether taking the hold-out set had any effect on the cross-validated RMSE
#   
# ## With the Hold-out Set
# 
# By testing the performance with the hold-out set, we can see the effects that 
# the tuning had over the two baseline models: 

holdout_x <- predict(oneHot, holdOut %>% select(one_of(features))) %>% 
  as.data.frame(.) %>% 
  select(inputs)

holdout_y <- holdOut$value

(xgb_base_rmse <- ModelMetrics::rmse(holdout_y, predict(xgb_base, newdata = holdout_x)))
(xgb_model_rmse <- ModelMetrics::rmse(holdout_y, predict(model, newdata = holdout_x)))


output <- tibble(Actual = exp(holdout_y), Predicted = exp(predict(model, holdout_x)), Param = 'secchi', log = 'ln', uniqueID = holdOut$uniqueID)

save(model, file = 'models/ffs_noBlu_ln_secchi.Rdata')
```

```{r Eval}
evals <- output %>%
  group_by(Param, log) %>%
  summarise(rmse = rmse(Actual, Predicted),
            mae = mae(Actual, Predicted),
            mape = mape(Actual, Predicted),
            bias = bias(Actual, Predicted),
            p.bias = percent_bias(Actual, Predicted),
            smape = smape(Actual, Predicted)) %>%
  mutate(iteration = iteration)

evals %>% kable(digits = 2) %>% kable_styling() %>% scroll_box(width = '4in')

ggplot(output %>% filter(log == e$log), aes(x = Actual, y = Predicted)) + 
  geom_hex(aes(fill = ..count..)) + 
  scale_fill_viridis(name = 'Point\nCount', trans = 'log10') + 
  geom_abline(color = 'red') + 
  stat_poly_eq(aes(label =  paste(stat(adj.rr.label))),
               formula = y~x, parse = TRUE, 
               label.y = Inf, vjust = 1.3) +
  scale_x_continuous(trans = 'log10', labels = scales::comma) +
  scale_y_continuous(trans = 'log10', labels = scales::comma) +
  #coord_equal(ratio = 1) +
  facet_wrap(~Param, scales = 'free', shrink = T) +
  labs(title = 'XgBoost Water Quality Models', subtitle = 'Red line is 1:1')

output.full <- output %>%
  #filter(log == 'ln') %>%
  left_join(srMunged, by = c('uniqueID')) %>%
  mutate(residual = Actual - Predicted,
         year = year(date),
         month = month(date))

## Some weird hacks for reordering factors
reorder_within <- function(x, by, within, fun = mean, sep = "___", ...) {
  new_x <- paste(x, within, sep = sep)
  stats::reorder(new_x, by, FUN = fun)
}

scale_x_reordered <- function(..., sep = "___") {
  reg <- paste0(sep, ".+$")
  ggplot2::scale_x_discrete(labels = function(x) gsub(reg, "", x), ...)
}
# 
# ## Summary Error metrics
# ## By actual value quantile
# errorSum <- output.full %>%
#   group_by(Param, log) %>%
#   mutate(quantile = cut_number(Actual, 10, right = F, labels = F),
#          quantLabs = cut_number(Actual, 10,  right = F, dig.lab = 1)) %>%
#   ungroup() %>%
#   group_by(Param, log, quantile, quantLabs) %>%
#   summarise(rmse = rmse(Actual, Predicted),
#             #mae = mae(Actual, Predicted),
#             bias = bias(Actual, Predicted))%>%
#   gather(rmse:bias, key = 'Metric', value = 'Value') %>%
#   arrange(Param, log, Metric, quantile) %>%
#   as.data.frame() %>%
#   mutate(order = row_number())
# 
# ggplot(errorSum, aes(x = reorder_within(quantLabs, quantile, Param), y = Value, color = Metric, linetype = log, group = interaction(Metric, Param, log))) +
#   geom_point() +
#   geom_line() +
#   scale_x_reordered() +
#   facet_wrap(~Param, scales = 'free') +
#   theme_bw() +
#   theme(axis.text.x = element_text(angle = 90, vjust = .5)) +
#   labs(title = 'Error by In Situ Value binned into 10% Quantiles') +
#   xlab('Binned Field Obs')


## Summary metrics across space time and observed value
errorSum <- output.full %>%
  #filter(Param == 'secchi') %>%
  mutate(Observed.Value = Actual) %>%
  rename(Year = year, Latitude = lat, Longitude = long) %>%
  gather(Observed.Value, Year, Latitude, Longitude, key = 'Variable', value = 'Value') %>%
  group_by(Variable, Param, log) %>%
  mutate(quantile = cut_number(Value, 10, right = F, labels = F),
         quantLabs = cut_number(Value, 10,  right = F, dig.lab = 1)) %>%
  ungroup() %>%
  group_by(quantile, quantLabs, Param, Variable, log) %>%
  dplyr::summarise(mae = mae(Actual, Predicted),
            smape = smape(Actual, Predicted),
            p.bias = percent_bias(Actual, Predicted),
            #mae = mae(Actual, Predicted),
            bias = bias(Actual, Predicted)) %>%
  gather(mae:bias, key = 'Metric', value = 'Error') %>%
  as.data.frame() %>%
  arrange(Param, Variable, quantile) %>%
  mutate(order = row_number())


spaceTimeFigs('secchi')
spaceTimeFigs('secchi', abs = F)
# spaceTimeFigs('chl_a')
# spaceTimeFigs('chl_a', abs = F)
# spaceTimeFigs('tss')
# spaceTimeFigs('tss', abs = F)

xgb_model$modelInfo$varImp(xgb_model$finalModel) %>%
  mutate(Feature = fct_reorder(rownames(.), Overall, .desc = T)) %>%
  arrange(Overall) %>%
  ggplot(., aes(x = Feature, y = Overall)) + geom_col() + coord_flip()
  

```



```{r, fig.height= 8}
output.full <- output.full %>%
  filter(Param == 'secchi', log == e$log) %>%
  st_as_sf(coords = c('long','lat'), crs = 4326) %>% 
  st_join(region %>% st_transform(.,4326)) 

output.full %>% 
  group_by(sat) %>%
  summarise(mae = mae(Actual, Predicted),
            smape = smape(Actual, Predicted),
            p.bias = percent_bias(Actual, Predicted),
            #mae = mae(Actual, Predicted),
            bias = bias(Actual, Predicted)) %>%
  gather(mae:bias, key = 'Metric', value = 'Error') %>%
  ggplot(., aes(x = Metric, y = Error, fill = sat)) +
  geom_col(position = 'dodge') +
  theme_bw() + 
  ggtitle('Absolute and Relative Error by Satellite')
  
val.conf <- output.full %>%
  mutate(smape = 2*(abs(Actual - Predicted)/(abs(Actual) + abs(Predicted))),
         p.bias = (Actual - Predicted)/abs(Actual)) %>%
  group_by(region, sat, log) %>%
  summarise(smape = sd(smape),
            p.bias = sd(p.bias)) %>%
  mutate(p.bias = ifelse(is.na(p.bias), 0, p.bias),
         smape = ifelse(is.na(smape), 0, smape)) %>%
  gather(smape, p.bias, key = 'Metric', value = 'sd') %>%
  st_set_geometry(NULL)
  
output.full %>%
  group_by(region, sat, log) %>%
  summarise(smape = smape(Actual, Predicted),
            p.bias = percent_bias(Actual, Predicted),
            count = n()) %>%
  gather(smape:p.bias, key = 'Metric', value = 'value') %>%
  left_join(val.conf %>% select(region, sat, Metric, sd)) %>%
  ggplot(.) +
  scale_fill_viridis_c(trans = 'log10') +
  #geom_text(x = .5, y = .5) +
  geom_col(position = 'dodge', aes(x = Metric, y = value, fill = count, color = sat, group = sat)) +
  geom_errorbar(position = position_dodge(.9), aes(x = Metric, ymin = value - sd, ymax = value + sd, group = sat), width = .2) +
  #scale_y_continuous(labels = scales::percent) +
  theme(legend.position = 'top') +
  ggtitle('Error by region') +
  facet_wrap(~region, scales = 'free')#, ncol = 4)

ggplot(output.full %>% filter(log == e$log), aes(x = Actual, y = Predicted)) + 
  geom_point(alpha = .3) +
  geom_smooth(method = 'lm', aes(color = sat)) +
  #scale_fill_viridis(name = 'Point\nCount', trans = 'log10') + 
  #geom_abline(color = 'red') + 
  stat_poly_eq(aes(label =  paste(stat(adj.rr.label))),
               formula = y~x, parse = TRUE, 
               label.y = Inf, vjust = 1.3) +
  scale_x_continuous(trans = 'log10', labels = scales::comma) +
  scale_y_continuous(trans = 'log10', labels = scales::comma) +
  #coord_equal(ratio = 1) +
  facet_wrap(~region, scales = 'free', shrink = T) +
  labs(title = 'Regional Evaluation Performance')

dummy <- expand.grid(year = seq(1984,2018), month = c(1:12), region = factor(unique(region$region)))

fieldmatch <- output.full %>%
  filter(year < 2019,
         !is.na(region)) %>%
  rename(RS = Predicted, Field = Actual) %>%
  gather(RS,Field, key = 'Measure', value = 'value') %>%
  group_by(year, Measure, month, region) %>%
  summarise(median = median(value, na.rm = T),
            sd = sd(value, na.rm =T),
            count.obs = n())%>%
  right_join(dummy)%>%
  arrange(region,year, month)%>%
  mutate(date = as.POSIXct(paste0(year,'/',month,'/',01)))

ggplot(fieldmatch %>% na.omit(), aes(date, y = median, color = Measure)) + 
  geom_point(alpha = .1) +
  geom_smooth(method = 'loess', span = .3, se = F) + 
  facet_wrap(~region, scales = 'free', ncol = 4) +
  labs(title = 'Smoothed SDD Trends By Region Using Coincident Testing Data') +
  theme_bw() +
  theme(legend.position = 'bottom')

```

### Visual Evalution of ~10 randomly selected lakes

These lakes were removed from train/test data.  The in situ vs satellite data are +/-1 month matchups.  Also, field measurements are coming from a single point whereas RS predictions are based on median reflectance values for the entire lake.

```{r, fig.height = 8}
## Big plots
##Read in the data to look
#ts.full <- read_feather('out/macroSystems/dsweStrictUnGroupedOptvarWeighted_MSxgboostPreds.feather')
## Take a quick look.

lakesDown <- list.files('out/EvalSites/qaPull', full.names = T)

#Extract vector of empty files' names
empties <- lakesDown[file.info(lakesDown)[["size"]]==1]
lakesDown <- lakesDown[!lakesDown %in% empties]

sites.eval <- read_feather('out/evalSites.feather')
fieldObs <- read_feather('out/evalFieldObs.feather')

# Map over the pulled reflectance values and calculate time-series predictions for each constituent
ids <- unique(as.character(sites.eval$COMID))

ts.full <-  ids %>% purrr::map_dfr(~EvalPreds(.,lakesDown, sites.eval, 'secchi', log, model = xgb_model$finalModel, features = inputs))

ts.full <- ts.full %>%
  mutate(source = 'RS',
         date = as_date(ymd_hms(date)),
         week = week(date)) %>%
  select(date, year, week, source, value, parameter, COMID) 


fieldObs <- fieldObs %>% 
  filter(parameter == 'secchi') %>%
  mutate(date = as_date(ymd_hms(date_unity)),
  year = year(date),
  week = week(date),
  source = 'inSitu') %>%
  select(date, year, week, source, value, parameter, COMID) %>%
  filter(year > 1984,
         !(value > 20 & parameter == 'secchi'))

lagosRS.join <- ts.full %>%
  bind_rows(fieldObs %>% group_by(date, parameter, COMID, source, week, year) %>%
              summarise(value = mean(value))) %>%
  mutate(siteParam = paste0(parameter,'_', COMID),
         myID = paste0(year(date),month(date), COMID, parameter))

fieldObsdates <- lagosRS.join %>% filter(source == 'inSitu') %>% select(myID)

ts.out <- lagosRS.join %>% filter(myID %in% fieldObsdates$myID)

ggplot(ts.out) + geom_point(aes(x = date, y = value, color = source, shape = parameter), alpha = .7) +
  geom_smooth(aes(x = date, y = value, linetype = source), method = 'loess', span = .3, se = F) +
  #scale_color_viridis_d() +
facet_wrap(~siteParam, scales = 'free', ncol = 4) + 
  theme(strip.background = element_blank(), strip.text.x = element_blank(), legend.position = 'top') +
  labs(title = 'Field vs Predicted Observations Over Time')


ggplot(ts.out) + geom_density(aes(x = value, fill = source, color = parameter), alpha = .5, size = .7) +
  scale_color_viridis_d() +
  facet_wrap(~siteParam, scales = 'free', ncol = 4) + 
  theme(
  strip.background = element_blank(),
  strip.text.x = element_blank(),
  legend.position = 'top') +
  labs(title = 'Field vs Predicted Observations Distributions')
```


```{r, eval = F}
ts.match <- ts.full %>%
  inner_join(fieldObs, by = c('year', 'week', 'COMID', 'parameter')) %>%
  rename(date.rs = date.x, date.is = date.y, value.rs = value.x, value.is = value.y) %>%
  mutate(date.diff = as.numeric(date.rs - date.is),
         siteParam = paste0(parameter,'_', COMID)) %>%
  filter(abs(date.diff) < 5)

ggplot(ts.match, aes(x = value.is, y = value.rs)) + 
  geom_point(aes(color = COMID)) + 
  geom_abline(color = 'red') + 
  stat_poly_eq(aes(label =  paste(stat(adj.rr.label))),
               formula = y~x, parse = TRUE, 
               label.y = Inf, vjust = 1.3) +
  scale_x_log10() +
  scale_y_log10() +
   theme(
   legend.position = 'none') +
  facet_wrap(~parameter, scales = 'free', shrink = T) +
  labs(title = 'Evaluation Match-Up Data', subtitle = 'Red line is 1:1')


check <- fieldObs %>%
  group_by(date, parameter, COMID) %>%
  summarise(count = n(),
            mean = mean(value),
            sd = sd(value)) %>%
  na.omit()

ggplot(check, aes(x = mean, y = sd)) + geom_point(aes(color = parameter)) + xlim(0,10) + ylim(0,5) + labs(title = 'Mean and St.Dev of Same-Day in-situ observations')
```

