---
title: "2_model_training"
author: "Simon Topp"
date: "11/8/2019"
output: html_document
---


#Attempt at FFS Using xgboost and spatial cross validation.

```{r}
## Do some final munging/feature engineering
# srMunged <- read_feather('out/srMunged.feather') %>%
#   filter(parameter == 'secchi',
#          value <= 10,
#          !is.na(CatAreaSqKm)) %>% #100 obs that didn't match up with LakeCat
#   mutate(COMID = as.character(COMID),
#          pctForest2006 = PctDecid2006Cat + PctConif2006Cat + PctMxFst2006Cat,
#          pctUrban2006 = PctUrbMd2006Cat + PctUrbHi2006Cat,
#          pctWetland2006 = PctWdWet2006Cat + PctHbWet2006Cat,
#          areasqkm = round(areasqkm, 1),
#          meandused = round(meandused, 1),
#          inStreamCat = as.numeric(inStreamCat),
#          #Correct the reflectance values based on distribution comparisons (see line 358 in 1_nhd_join_and_munge.Rmd)
#          blue = ifelse(sat == '5', blue - .84, ifelse(sat == '8', blue + 76.3, blue)),
#          green = ifelse(sat == '5', green - 29.2, ifelse(sat == '8', green + 46.9, green)),
#          nir = ifelse(sat == '5', nir - 34.3, ifelse(sat == '8', nir + 127, nir)),
#          red = ifelse(sat == '5', red - 26.1, ifelse(sat == '8', red + 51.5, red)),
#          NR = nir/red,
#          BG = blue/green,
#          dWL = fui.hue(red, green, blue)) %>%
#   filter_at(vars(nir, red, blue, green), all_vars(.>0 & .<2000)) %>%
#   rowwise() %>%
#   mutate(saturation = rgb2hsv(r=red, g=green, b=blue, maxColorValue = 2000)[2],
#          brightness = rgb2hsv(r=red,g= green, b= blue, maxColorValue = 2000)[3]) %>%
#   ungroup()




## Select parameters to go into model
paste0(names(srMunged), collapse = ', ')

## These parameters make sense in terms of their mechanisms, resolution (with some assumptions), and reliability
features <- "NR, BG, dWL, inStreamCat, PctCarbResidCat, Precip8110Cat, Tmean8110Cat, RunoffCat, ClayCat, SandCat, OmCat, PermCat, RckdepCat, WtDepCat, BFICat, ElevCat, AOD, BFICat, KffactCat, CatAreaSqKm, PctAg2006Slp10Cat, ElevCat, NPDESDensCat, HydrlCondCat, PctImp2006Cat, pctUrban2006, pctForest2006, PctCrop2006Cat, pctWetland2006, WetIndexCat, areasqkm, meandused"

features <- str_split(features, pattern = ', ') %>% unlist()

##Pull holdout data
set.seed(340987)
holdOut <- srMunged %>%
  group_by(region) %>%
  sample_frac(.2)

df <- srMunged %>% filter(!UniqueID %in% holdOut$UniqueID)

#One hot encoding for xgboost
oneHot <- onehot(df %>% select(features))
df.encoded <- predict(oneHot, df %>% select(features)) %>% as.data.frame(.) 

# Split up cross validation folds based on stratified sampling
df <- df %>% mutate(SpaceTime = paste0(region, sat),
                    group1 = cut_number(lat, 3),
                    date = ymd(date),
                    julian = as.numeric(julian.Date(date))) %>%
            group_by(group1) %>%
            mutate(group2 = cut_number(long, 3)) %>%
            ungroup() %>%
            mutate(spaceCluster = paste0(group1,group2),
                   timeCluster = cut_number(julian, 9))

folds <- CreateSpacetimeFolds(df, spacevar = 'spaceCluster', timevar = 'timeCluster', k= 3, seed = 34985)

control <- trainControl(method="cv", savePredictions = 'none', 
                        returnResamp = 'final', index = folds$index, 
                        indexOut = folds$indexOut)

## Do initial feature selection with conservative hyperparameters
ffs.tuneGrid <- expand.grid(nrounds = 300, max_depth = 4, eta = .1, 
                            gamma = 1, min_child_weight = 1, colsample_bytree = .6,
                            subsample = .6)

# Takes awhile, so set it up all to run in parrallel
cl <- makePSOCKcluster(availableCores() - 4)
registerDoParallel(cl)

ffs <- ffs(df.encoded, df$value, method = 'xgbTree', metric = 'RMSE', 
           tuneGrid = ffs.tuneGrid, trControl = control, verbose = T)

stopCluster(cl)

ffsResults <- ffs$perf_all

# Save the results
write_feather(ffsResults, 'out/ffsResultsFull.feather')

ffsResults %>%
  group_by(nvar) %>%
  summarise(RMSE = median(RMSE),
            SE = median(SE)) %>%
  ggplot(.) + geom_line(aes(x = nvar, y = RMSE)) +
  geom_errorbar(aes(x = nvar, ymin = RMSE - SE, ymax = RMSE + SE), color = 'red')

#ggsave('figures/rfeRMSE.png', device = 'png', width = 6, height = 4, units = 'in')
```

### XGBoost with Default Hyperparameters

```{r baseline model, message=FALSE}
ffsResults <- read_feather('out/ffsResultsFull.feather')
features <- ffsResults[ffsResults$RMSE == min(ffsResults$RMSE),] %>%
  select(-c(nvar, RMSE, SE)) %>%
  paste(.) %>% .[.!= 'NA']

oneHot <- onehot(df %>% select(features))
df.encoded <- predict(oneHot, df %>% select(features)) %>% as.data.frame(.) 

# We'll set up a tuning grid with default values, define a `trainControl()`
# function (which in this case disables printing training log to console
# plus disables resampling as we want to use all training data to fit the
# model), and finally fit the model with `caret::train()`:

grid_default <- expand.grid(
  nrounds = 100,
  max_depth = 4,
  eta = 0.3,
  gamma = 0,
  colsample_bytree = .8,
  min_child_weight = .8,
  subsample = 1
)

#Set up a cluster to run everything in parrallel
cl <- makePSOCKcluster(availableCores()/2)
registerDoParallel(cl)

train_control <- caret::trainControl(method="cv", savePredictions = T, 
                          returnResamp = 'final', index = folds$index, 
                          indexOut = folds$indexOut)

xgb_base <- caret::train(
  x = df.encoded,
  y = df$value,
  trControl = train_control,
  tuneGrid = grid_default,
  method = "xgbTree",
  verbose = TRUE
)
```

# Grid Search for Hyperparameters

```{r First tune, warning=FALSE, fig.height=7}
# Next, we build tuning grid for `caret` to explore between different
# hyperparameter values. To get started we're using some suggestions from
# [here](https://www.slideshare.net/OwenZhang2/tips-for-data-science-competitions/14).
# 
# ## Step 1: Number of Iterations and the Learning Rate
# 
# We going to start the tuning "the bigger knobs" by setting up the maximum
# number of trees:

# note to start nrounds from 200, as smaller learning rates result in errors so
# big with lower starting points that they'll mess the scales


# To get reasonable running time while testing hyperparameter combinations
# with `caret` we don't want to go over `r nrounds`. Then, we want to find a good enough learning rate for this number of trees,
# as for lower learning rates `r nrounds` iterations might not be enough.
# 
# Next, as the maximum tree depth is also depending on the number of iterations
# and the learning rate, we want to experiment with it at this point to narrow
# down the possible hyperparameters. We'll also create a helper function to 
# create the visualizations with `ggplot2`, called `tuneplot()`:

nrounds <- 1000

tune_grid <- expand.grid(
  nrounds = seq(from = 100, to = nrounds, by = 100),
  eta = c(0.05, 0.1, 0.3),
  max_depth = c(2, 3, 4),
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

tune_control <- caret::trainControl(
  method = "cv", # cross-validation
  index = folds$index,
  indexOut = folds$indexOut,# with n folds 
  verboseIter = FALSE, # no training log
  allowParallel = TRUE # FALSE for reproducible results 
)

xgb_tune <- caret::train(
  x = df.encoded,
  y = df$value,
  trControl = tune_control,
  tuneGrid = tune_grid,
  method = "xgbTree",
  verbose = TRUE
)

# helper function for the plots
tuneplot <- function(x, probs = .90) {
  ggplot(x) +
    coord_cartesian(ylim = c(quantile(x$results$RMSE, probs = probs), min(x$results$RMSE))) +
    theme_bw()
}

tuneplot(xgb_tune)
xgb_tune$bestTune
```


```{r second tune, warning=FALSE, fig.height=5}

# With `r nrounds` iterations a learning rate of `r xgb_tune$bestTune$eta`
# seems to be a good starting point. The model currently has a cross-validated
# RMSE of `r round(min(xgb_tune$results$RMSE), digits = 5)`. Also to be noted 
# that with `max_depth = 2` and smaller learning rates the forest seems not yet
# stabile.
# 
# ## Step 2: Maximum Depth and Minimum Child Weight
# 
# After fixing the learning rate to `r xgb_tune$bestTune$eta` and we'll also
# set maximum depth to `r xgb_tune$bestTune$max_depth` +-1 (or +2 if 
# `max_depth == 2`) to experiment a bit around the suggested best tune in
# previous step. Then, well fix maximum depth and minimum child weight:


tune_grid2 <- expand.grid(
  nrounds = seq(from = 50, to = nrounds, by = 50),
  eta = xgb_tune$bestTune$eta,
  max_depth = ifelse(xgb_tune$bestTune$max_depth == 2,
    c(xgb_tune$bestTune$max_depth:4),
    xgb_tune$bestTune$max_depth - 1:xgb_tune$bestTune$max_depth + 1),
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = c(1, 2, 3),
  subsample = 1
)

xgb_tune2 <- caret::train(
  x = df.encoded,
  y = df$value,
  trControl = tune_control,
  tuneGrid = tune_grid2,
  method = "xgbTree",
  verbose = TRUE
)

tuneplot(xgb_tune2)
xgb_tune2$bestTune
```


```{r third tune, warning=FALSE, fig.height=5}
# The second tune results RMSE of `r round(min(xgb_tune2$results$RMSE), digits=5)`,
# `r glue("{round((min(xgb_tune$results$RMSE)/min(xgb_tune2$results$RMSE) - 1)*100, digits=2)}%")`
# over the previous step.
# 
# ## Step 3: Column and Row Sampling
# Based on this, we can fix minimum child weight to `r xgb_tune2$bestTune$min_child_weight`
# and maximum depth to `r xgb_tune2$bestTune$max_depth`. Next, we'll try
# different values for row and column sampling:

tune_grid3 <- expand.grid(
  nrounds = seq(from = 50, to = nrounds, by = 50),
  eta = xgb_tune$bestTune$eta,
  max_depth = xgb_tune2$bestTune$max_depth,
  gamma = 0,
  colsample_bytree = c(0.4, 0.6, 0.8, 1.0),
  min_child_weight = xgb_tune2$bestTune$min_child_weight,
  subsample = c(0.5, 0.75, 1.0)
)

xgb_tune3 <- caret::train(
  x = df.encoded,
  y = df$value,
  trControl = tune_control,
  tuneGrid = tune_grid3,
  method = "xgbTree",
  verbose = TRUE
)

tuneplot(xgb_tune3, probs = .95)
xgb_tune3$bestTune
```

```{r fourth tune, warning=FALSE, fig.height=5}

# Based on the above, we can set column sampling to `r xgb_tune3$bestTune$colsample_bytree`
# and row sampling to `r xgb_tune3$bestTune$subsample`. RMSE at this point is `r round(min(xgb_tune3$results$RMSE), digits=5)`, which is
# `r glue("{round((min(xgb_tune2$results$RMSE)/min(xgb_tune3$results$RMSE)-1) * 100, digits=2)}%")`
# better than RMSE in step 2.
# 
# ## Step 4: Gamma
# Next, we again pick the best values from previous step, and now will see 
# whether changing the gamma has any effect on the model fit:
# 


tune_grid4 <- expand.grid(
  nrounds = seq(from = 50, to = nrounds, by = 50),
  eta = xgb_tune$bestTune$eta,
  max_depth = xgb_tune2$bestTune$max_depth,
  gamma = c(0, 0.05, 0.1, 0.5, 0.7, 0.9, 1.0),
  colsample_bytree = xgb_tune3$bestTune$colsample_bytree,
  min_child_weight = xgb_tune2$bestTune$min_child_weight,
  subsample = xgb_tune3$bestTune$subsample
)

xgb_tune4 <- caret::train(
  x = df.encoded,
  y = df$value,
  trControl = tune_control,
  tuneGrid = tune_grid4,
  method = "xgbTree",
  verbose = TRUE
)

tuneplot(xgb_tune4)
xgb_tune4$bestTune
```


```{r fifth tune, warning=FALSE}
# After the inspection, we can set the gamma to `r xgb_tune4$bestTune$gamma`. 
# RMSE of this model is `r round(min(xgb_tune4$results$RMSE), digits=5)`
# `r if(xgb_tune4$bestTune$gamma!=0){glue("which is {round((min(xgb_tune3$results$RMSE)/min(xgb_tune4$results$RMSE)-1)*100, digits=2)}% improvement.")}else{"."}`
#
# ## Step 5: Reducing the Learning Rate
# 
# Now, we have tuned the hyperparameters and can start reducing the learning
# rate to get to the final model:

tune_grid5 <- expand.grid(
  nrounds = seq(from = 100, to = 1000, by = 50),
  eta = c(0.05, 0.1, .15), #c(0.01, 0.015, 0.025, 0.05, 0.1),
  max_depth = xgb_tune2$bestTune$max_depth,
  gamma = xgb_tune4$bestTune$gamma,
  colsample_bytree = xgb_tune3$bestTune$colsample_bytree,
  min_child_weight = xgb_tune2$bestTune$min_child_weight,
  subsample = xgb_tune3$bestTune$subsample
)

xgb_tune5 <- caret::train(
  x = df.encoded,
  y = df$value,
  trControl = tune_control,
  tuneGrid = tune_grid5,
  method = "xgbTree",
  verbose = TRUE
)

tuneplot(xgb_tune5)
xgb_tune5$bestTune
```


```{r final model, comment=NA}

# These will be the hyperparameters for the final model. RMSE now sets to 
# `r round(min(xgb_tune5$results$RMSE), digits=5)`, resulting in improvement of
# `r glue("{round((min(xgb_tune4$results$RMSE)/min(xgb_tune5$results$RMSE)-1)*100, digits=2)}%")`
# over the step 4, which in total is
# `r glue("{round((min(xgb_tune$results$RMSE)/min(xgb_tune5$results$RMSE)-1)*100, digits=2)}%")`
# better than the first tune.
# 
# ## Fitting the Model
# 
# Now that we have determined the parameters we want to use, we will use the
# training data (excluding the hold-out set which we will soon use to measure
# the model performance) without resampling to fit the model:


(final_grid <- expand.grid(
  nrounds = xgb_tune5$bestTune$nrounds,
  eta = xgb_tune5$bestTune$eta,
  max_depth = xgb_tune5$bestTune$max_depth,
  gamma = xgb_tune5$bestTune$gamma,
  colsample_bytree = xgb_tune5$bestTune$colsample_bytree,
  min_child_weight = xgb_tune5$bestTune$min_child_weight,
  subsample = xgb_tune5$bestTune$subsample
))

# cl <- makePSOCKcluster(6)
# registerDoParallel(cl)

(model <- caret::train(
  x = df.encoded,
  y = df$value,
  trControl = train_control,
  tuneGrid = final_grid,
  method = "xgbTree",
  verbose = TRUE
))

stopCluster(cl)
```


```{r hold-out RMSE}

# Note, that this model is not trained with whole training set yet which we will
# still do before submitting the predictions.
# 
# # Evaluating the Model Performance
# 
# Now we will evaluate the model performance by:
# 
#  - Testing the performance of the two baseline models and the final model with the hold-out data
#  - Submitting predictions to Kaggle from default hyperparameter XGBoost plus the tuned one 
#  - Testing whether taking the hold-out set had any effect on the cross-validated RMSE
#   
# ## With the Hold-out Set
# 
# By testing the performance with the hold-out set, we can see the effects that 
# the tuning had over the two baseline models: 

holdout_x <- predict(oneHot, holdOut %>% select(features))

holdout_y <- holdOut$value

if(log == T){
output <- tibble(Actual = exp(holdout_y), Predicted = exp(predict(model, holdout_x)), log = 'ln', UniqueID = holdOut$UniqueID)
}else{output <- tibble(Actual = holdout_y, Predicted = predict(model, holdout_x), log = 'none', UniqueID = holdOut$UniqueID)}

save(model, file = paste0('models/',iteration, '.Rdata'))
write_feather(output, paste0('out/outputs/',iteration, '.feather'))
```