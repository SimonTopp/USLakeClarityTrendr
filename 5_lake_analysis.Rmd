---
title: "7.5_LakeModellingCorrelations"
author: "Simon Topp"
date: "4/17/2019"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup}
knitr::opts_chunk$set(echo = F)
knitr::opts_chunk$set(message = F)
knitr::opts_chunk$set(warning = F)
knitr::opts_chunk$set(results = 'hide')

library(tidyverse)
library(feather)
library(viridis)
library(knitr)
library(sf)
library(magrittr)
library(rkt)
library(lubridate)
library(broom)
library(furrr)
library(plotly)
library(corrgram)
library(stlplus)
library(gganimate)
#library(raster)
library(rgdal)
library(gridExtra)
library(scales)
library(trend)
library(kableExtra)
library(Hmisc)
library(boot)
```

# This document details the primary results of regionalized bootstrapping for remotely sensed lake water clarity timeseries.

```{r}
# Bring in all the necessary stoof
iteration = 'FFS_DistNorm_noLn_Secchi'
lakeSamp = 'EcoReg2000'
log = F
region = st_read('in/NLA/NLA_Ecoregions/EcoRegsMerged.shp')
load(paste0('models/',iteration,'.Rdata'))
source('0_ModellingFunctions.R')

Preds.out <- read_feather(paste0('out/TS_Preds/', lakeSamp, '_',iteration,'.feather'))

if(lakeSamp == 'NLA'){
  lake.join <- read_feather('out/NLA2012LakesFull.feather')
  }else if(lakeSamp == 'EcoReg2000'){
    lake.join <- read_feather('out/EcoReg2000LakesFull.feather')
  }

```


## First, compare the predictions with NLA values as a final validation

```{r}
nla.2007 <- read.csv('in/NLA/nla2007_secchi_20091008.txt', stringsAsFactors = F) %>%
  select(SITE_ID, secchi = SECMEAN, date = DATE_SECCHI) %>%
  left_join(read.csv('in/NLA/nla2007_sampledlakeinformation_20091113.txt') %>%
              select(SITE_ID, COMID = COM_ID, region = WSA_ECO9, 
                     lat = LAT_DD, long = LON_DD))

nla.2012 <- read.csv('in/NLA/nla2012_secchi_08232016.txt', stringsAsFactors = F) %>%
  select(SITE_ID, secchi = SECCHI, date = DATE_COL) %>%
  left_join(read.csv('in/NLA/nla2012_wide_siteinfo_08232016.txt') %>%
              select(SITE_ID, COMID = COMID2012, region = AGGR_ECO9_2015, 
                     lat = LAT_DD83, long = LON_DD83))

nlaFull <- nla.2007 %>%
  mutate(nla.year = 2007) %>%
  bind_rows(nla.2012 %>% mutate(nla.year = 2012)) %>%
  mutate(date = mdy(date),
         year = year(date),
         month = month(date),
         region = factor(region, 
                         labels = c("Coastal Plain", "Northern Appalachians", "Northern Plains", "Southern Appalachians", "Southern Plains", "Temperate Planes", "Upper Midwest","Western Mountains","Xeric West"))) %>%
  group_by(SITE_ID, region, nla.year) %>%
  summarise(secchi = median(secchi, na.rm = T)) %>%
  group_by(region, nla.year) %>%
  rename(year = nla.year) %>%
  summarise(secchi.mean = mean(secchi, na.rm = T),
            secchi.median = median(secchi, na.rm = T),
            secchi.sd = sd(secchi, na.rm =T)) %>%
  mutate(source = 'NLA')

predComp <- Preds.out %>% 
  mutate(source = 'Landsat') %>%
  na.omit() %>%
  filter(month %in% c(5:9), year %in% c(2007,2012)) %>%
  group_by(year, region, source) %>%
  summarise(secchi.mean = mean(value, na.rm = T),
            secchi.median = median(value, na.rm = T),
            secchi.sd = sd(value, na.rm =T)) %>%
  bind_rows(nlaFull)

ggplot(predComp, aes(x = region, color = source)) +
  geom_point(aes(y = secchi.mean)) +
  #geom_point(aes(y = secchi.median)) +
  geom_errorbar(aes(ymin = secchi.mean - secchi.sd, ymax = secchi.mean + secchi.sd)) +
  scale_color_viridis_d(end = .7) +
  facet_wrap(~year) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(x = 'Region', y = 'Water Clarity (m)', title = 'Comparison of NLA to Remotely Sensed Predictions')

ggsave('figures/NLAComps.png', width = 5, height = 4.5, units = 'in')

check <- predComp %>% select(-secchi.median, -secchi.sd) %>% spread(source, secchi.mean)
rmse(check$NLA, check$Landsat)
bias(check$NLA, check$Landsat)
mape(check$NLA, check$Landsat)
mae(check$NLA, check$Landsat)
```

## Pull out median summer values first by lake and then region to examine non-monotic trends.

```{r}
## Take a quick look at mean trends in each HUC2

#How many lakes failed? The way the code is written we'll have 1 row of NA's for each lake that failed
colSums(is.na(Preds.out)) ##Only 39 out of ~2.5k, not bad

##Look at summer median values
Preds.out%>%
  left_join(lake.join %>% select(COMID, areasqkm)) %>%
  mutate(sizeClass = ifelse(areasqkm > 100,'big','small')) %>%
  na.omit() %>%
  filter(month %in% c(5:9)) %>%
  group_by(year, region) %>%
  summarise(value = mean(value)) %>%
  ggplot(., aes(x = year, y = value, color = region)) +
  geom_line() +
  geom_point() +
  labs(y = 'SDD (m)', title = 'SDD by HUC2 over time')

#Create dataframe of median summer values for each lake plus a mk-test stats
summ.meds <- Preds.out %>%
  filter(month %in% c(5:9),
         year < 2019) %>% ##Incomplete obs for 2019 so we won't use it.
  group_by(COMID, year, region) %>%
  summarise(secchi.med = median(value))

##Check how many years we have observations for each lake and only take lakes with > 10 years of obs. We loose ~500 lakes.
counts <- summ.meds %>%
  group_by(COMID) %>%
  summarise(count = n()) %>%
  filter(count > 10) %>%
  left_join(lake.join)

####  Create wide dataset 
summ.meds.wide <- summ.meds %>%
  filter(COMID %in% counts$COMID) %>%
  spread(year, secchi.med)

##Check regional counts to make sure they're reasonable
summ.meds.wide %>% group_by(region) %>% summarise(count = n())

###### Apply 1000 rounds of bootstrapping to yearly summer medians

## Map over the regions and pull out summary stats
bootstrapped.ts <- summ.meds.wide %>%
  group_by(region) %>%
  nest() %>%
  mutate(boot.means= purrr::map(data, ~boot(.,boot.med, R = 1000)),
         boots.summ = purrr::map(boot.means, boot.summary)) %>%
  select(-boot.means, -data) %>%
  unnest(boots.summ)

## Generate a figure showing the mean summer clarity and 'stability' (sd of bootstrap iterations) for each lake.
bootstrapped.ts %>%
  filter(year < 2019, year > 1984) %>%
  ggplot(., aes(x = year, y = mean)) +
  geom_errorbar(aes(ymin = mean - se, ymax = mean + se), color = 'grey60', alpha = .4) +
  geom_path(color = 'red') +
  theme_bw() +
  geom_point() +
  facet_wrap(~region, scales = 'free', ncol = 2) +
  labs(title = 'Mean and 90% confidence bound (stability)')

#####Look at MK stats for for each region.
summaryMK <- bootstrapped.ts %>%
  na.omit() %>%
  group_by(region) %>%
  arrange(year) %>%
  nest() %>%
  mutate(mk = purrr::map(data, ~sens.slope(.$mean)),
         sen.slope = purrr::map_dbl(mk, 'estimates'),
         sen.slope = sen.slope*100,
         p.value = purrr::map_dbl(mk, 'p.value'),
         p.value = round(p.value, 5),
         sig = ifelse(p.value < 0.1, 'yes', NA)) %>%
  select(-data, -mk)

## Finally plot up Man-Kendal results
summaryMK %>%
  left_join(region) %>%
  rowwise() %>%
  mutate(coords.x = st_centroid(geometry)[1],
    coords.y = st_centroid(geometry)[2]) %>%
  ungroup() %>%
  ggplot() +
    geom_sf(aes(fill = sen.slope, color = sig)) +
    #geom_text(aes(x = coords.x, y = coords.y, label = region), color = 'white') +
    theme(axis.text.x = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks = element_blank(),
          rect = element_blank(),
          panel.grid.major = element_line(color = 'transparent'),
          legend.position = 'bottom') +
    scale_color_manual(values = 'red', breaks = 'yes') +
    scale_fill_viridis_c(option = 'cividis') +
    labs(fill = 'Slope (cm/year)', color = 'P.Value < 0.1', title = 'MannKendall Slopes') +
    guides(color = guide_legend(label = F))

write_feather(bootstrapped.ts, paste0('out/TS_Preds/',lakeSamp,'_bootstrapped.feather'))
write_feather(summaryMK, paste0('out/TS_Preds/',lakeSamp,'_summaryMK.feather'))
```


## Compare remotely sensed and in situ observations

```{r}
fieldMean <- read_feather('out/RegionalsInSituMeans.feather')

meanFull <- fieldMean %>% 
  mutate(source = 'In.Situ', se = NA, bias = NA) %>%
  bind_rows(read_feather('out/TS_Preds/NLA_bootstrapped.feather') %>%
              mutate(source = 'NLA')) %>%
  bind_rows(read_feather('out/TS_Preds/EcoReg2000_bootstrapped.feather') %>%
              mutate(source = 'Random'))


meanFull %>%
  filter(year < 2019, year > 1984) %>%
  filter(source != 'In.Situ') %>%
  ggplot(., aes(x = year, y = mean, group = source, color = source)) +
  geom_errorbar(aes(ymin = mean - se, ymax = mean + se), color = 'grey60', alpha = .8) +
  geom_line() +
  theme_bw() +
  #geom_point() +
  scale_color_viridis_d(end = .5) +
  facet_wrap(~region, scales = 'free', ncol = 3) +
  labs(title = 'Mean and 90% confidence bound (stability)', 
       y = 'Mean Summer Clarity (m)', x = 'Year') +
  theme(legend.position = 'bottom',# c(.75, .05),
        legend.direction = 'horizontal',
        axis.text.x = element_text(hjust = 1)) # c(0,0) bottom left, c(1,1) top-right.)

ggsave('figures/NLA_Random_TS_wide.png', width = 6, height = 5)

FieldMK <- fieldMean %>%
  group_by(region) %>%
  arrange(year) %>%
  nest() %>%
  mutate(mk = purrr::map(data, ~sens.slope(.$mean)),
         sen.slope = purrr::map_dbl(mk, 'estimates'),
         sen.slope = sen.slope*100,
         p.value = purrr::map_dbl(mk, 'p.value'),
         p.value = round(p.value, 5),
         sig = ifelse(p.value < 0.1, 'yes', NA)) %>%
  select(-data, -mk)


## Compare RS and in situ decompositions

mk.full <- FieldMK %>%
  mutate(source = 'In.Situ') %>%
  bind_rows(read_feather('out/TS_Preds/NLA_summaryMK.feather') %>%
              mutate(source = 'NLA Sample')) %>%
  bind_rows(read_feather('out/TS_Preds/EcoReg2000_summaryMK.feather') %>%
              mutate(source = 'Random Sample'))

trendDif <- mk.full %>%
  filter(source != 'In.Situ') %>%
  select(region, sen.slope, source) %>%
  spread(source, sen.slope) %>%
  mutate(sen.slope = NLA - Random) %>%
  mutate(sig = NA, p.value = NA, source = 'Difference') %>%
  left_join(region)

p1 <- mk.full %>%
  filter(source != 'In.Situ') %>%
  left_join(region) %>%
  ggplot() +
    geom_sf(aes(fill = sen.slope, color = sig)) +
    theme(axis.text.x = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks = element_blank(),
          rect = element_blank(),
          panel.grid.major = element_line(color = 'transparent'),
          legend.position = 'bottom') +
    scale_color_manual(values = 'red', breaks = 'yes') +
    scale_fill_viridis_c(option = 'cividis') +
    labs(fill = 'Slope (cm/year)', color = 'P.Value < 0.1', title = 'Thiel-Sen Slopes') +
    guides(color = guide_legend(label = F)) +
  facet_wrap(~source)

p2 <- ggplot(trendDif) +
  geom_sf(aes(fill = sen.slope), color = 'grey70') +
  theme(axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks = element_blank(),
        rect = element_blank(),
        panel.grid.major = element_line(color = 'transparent'),
        legend.position = 'bottom') +
  scale_fill_gradient2(low='#a50026',mid='#abd9e9', high='#313695', 
                       midpoint = 0, name = 'NLA minus Random\n(cm/yr)') +
  facet_wrap(~source)+
  ggtitle(' ')

g <- grid.arrange(p1,p2, ncol = 2, widths = c(2,1))

ggsave('figures/SampleComps.png', plot = g, width = 7.5, height = 4)

```

## Individual lake analysis

```{r}
#For individual lake analysis use stricter filter for total years with observations
counts <- summ.meds %>%
  group_by(COMID, region) %>%
  summarise(count = n()) %>%
  filter(count > 25)

##Calculate mann-Kendall statistics for each lake.
mk <- summ.meds %>% filter(COMID %in% counts$COMID) %>% 
  group_by(COMID, region) %>%
  arrange(year) %>%
  nest() %>%
  mutate(mk = purrr::map(data, ~sens.slope(.$secchi.med)),
         sen.slope = purrr::map_dbl(mk, 'estimates'),
         sen.slope = sen.slope*100,
         p.value = purrr::map_dbl(mk, 'p.value'),
         sig = ifelse(p.value < .1, T, F)) %>%
  select(COMID, region, sen.slope, p.value, sig) %>%
  left_join(lake.join) %>%
  mutate(sizeQuant = cut(areasqkm, breaks = c(0,.5,1,10,1000)))

ggplot(mk, aes(x = sen.slope)) + geom_density(aes(color = sizeQuant)) + facet_wrap(~region, scales = 'free')

mk %>% st_as_sf(., coords = c('long','lat'), crs = 4326) %>% st_transform(st_crs(region)) %>%
  ggplot(.) +   geom_sf(data = region) + geom_sf(aes(color = sen.slope, alpha = sig)) +
  scale_color_viridis_c() +
  scale_alpha_discrete(breaks = c(T,F), limits = c(1,.3), labels = c('True', 'False'))

```




##Quick look at how many obs we're averaging over for each month

```{r, include = F, eval = F}
## Average lake observations per region/month
monthCounts <- Preds.out %>%
  filter(year < 2019,
         month %in% c(5,6,7,8,9)) %>%
  group_by(COMID, year, region) %>%
  dplyr::summarize(count = n()) %>%
  group_by(year, region) %>%
  dplyr:: summarise(count.obs = sum(count),
            count.lakes = n())

#Make a 3 panel figure for how many lakes we observe each month and the total observations

quantiles <- tibble(quant.label = factor(seq(10,90,10)), 
                    quants = quantile(monthCounts$count.obs, c(seq(.1,.9,.1))))

p1 <- monthCounts %>%
  ggplot(., aes(x = count.obs)) +
  geom_histogram(binwidth = 10, color = 'black') +
  geom_vline(data = quantiles, aes(color = quant.label, xintercept = quants), size = 1, alpha = .6) +
  scale_x_continuous(breaks = pretty_breaks(n = 10)) +
  scale_color_viridis_d()+
  labs(x = 'Regional Observations Per Year', color = '10% Quantiles') +
  theme_bw() +
  theme(legend.position = 'none')

quantiles <- tibble(quant.label = factor(seq(10,90,10)), 
                    quants = quantile(monthCounts$count.lakes, c(seq(.1,.9,.1))))
p2 <- monthCounts %>%
  ggplot(., aes(x = count.lakes)) +
  geom_histogram(binwidth = 5, color = 'black', fill = 'grey80') +
  geom_vline(data = quantiles, aes(color = quant.label, xintercept = quants), size = 1, alpha = .6) +
  scale_color_viridis_d()+
  scale_x_continuous(breaks = pretty_breaks(n = 10)) +
  labs(x = 'Regional Lake Count Per Year', color = '10% Quantiles') +
  theme_bw() +
  theme(legend.position = 'bottom')

p3 <- Preds.out %>%
  filter(month %in% c(5:9)) %>%
  group_by(region, month, year) %>%
  summarise(count.obs = n()) %>%
  group_by(region, month) %>%
  summarise(count.obs = mean(count.obs)) %>%
  ggplot(., aes(x = region, y = count.obs, fill = factor(month))) +
  geom_col(position = 'dodge') +
  scale_fill_viridis_d()+
  labs(x = 'Region', y = 'Average Observations per Month', fill = 'Month') +
  theme_bw() +
  theme(legend.position = 'bottom') +
  guides(fill = guide_legend(nrow = 2))

g <- grid.arrange(p1,p2, p3, nrow = 3, heights = c(.3,.35,.35))

ggsave(plot = g, 'figures/MonthlyObs3Panel_IceFree.png', width = 7, height = 9, units = 'in')

## Take a look at individual lake ts and regional medians
ggplot() + 
  geom_line(data = Preds.out %>% na.omit(.), aes(group = COMID, x = ymd_hms(date), y = value, color = 'Individual Lake'), alpha = .2) + 
  geom_line(data = summ.meds %>% mutate(date = ymd(paste0(year,'-06-15'))), aes(x = date, y = secchi.med, color = 'Watershed Median')) +
  labs(x = 'Date', y = 'Water Clarity (m)', title = 'Individual Lake Timeseries', color = 'Time Series', fill = '') +
  scale_color_manual(values = c('black', 'red')) +
  facet_wrap(~region, scales = 'free') +
  theme_bw() +
  theme(legend.position = 'bottom')

#ggsave(paste0('figures/',mod,'_FullTSSE.png'), width =  7, height = 6, units = 'in')
```
```{r}

## Money figure, trends and seasonality across region

#Make a faceted figure (duplicate, just for for now)
# stl.boots.mean %>%
#   ggplot(., aes(x = date, y = trend_mean)) +
#     geom_linerange(aes(x = date, ymin = trend_mean - trend_sd, ymax = trend_mean + trend_sd), color = 'grey70') +
#     #geom_path(aes(x = date, y= seasonal_mean + trend_mean), color = 'grey50', size = .6) +
#     geom_path(color = 'red') +
#     theme_bw() +
#     facet_wrap(~region, scales = 'free_y') +
#     labs(y = 'Water Clarity (m)', x = 'Date')


# ## Money figure, trends and seasonality across region
# # Werid hack to get the facets in the correct places.
# regions <- tibble(region = c(paste0(region$HUC_2,'_',region$region), 
#                              '5.1_blank','5.2_blank', '7.1_blank', '7.2_blank'),
#                   hucs = c(region$HUC_2,5.1,5.2,7.1,7.2),
#                   region = c(as.character(region$region), 
#                              '5.1_blank','5.2_blank', '7.1_blank', '7.2_blank')) %>%
#   mutate(region = gsub(x = region, pattern = ' Region', replacement = ''),
#          region = factor(region, levels = unique(region[order(hucs)]))) %>%
#   select(-hucs) 
# 
# # Make a faceted figure
# p <- stl.boots.mean %>%
#   full_join(regions) %>%
#   ggplot(., aes(x = date, y = trend_mean)) +
#     geom_linerange(aes(x = date, ymin = trend_mean - trend_sd, ymax = trend_mean + trend_sd), color = 'grey70') +
#     #geom_path(aes(x = date, y= seasonal_mean + trend_mean), color = 'grey50', size = .6) +
#     geom_path(color = 'red') +
#     theme_bw() +
#     facet_wrap(~region,ncol = 4, scales = 'free_y') + 
#     labs(y = 'Water Clarity (m)', x = 'Date') +
#     theme(axis.text.x = element_text(hjust = .75),
#           strip.text = element_text(colour = 'white'))
# 
# g <- ggplot_gtable(ggplot_build(p))
# strip_both <- which(grepl('strip-', g$layout$name))
# 
# fills <- c(colors$fill[c(16,17,18,13:15,9:12,7)],'white','white',
#            colors$fill[c(8,5)],'white','white', colors$fill[c(6,1:4)])
# k <- 1
# for (i in strip_both[c(1,2,5:24)]) {
# j <- which(grepl('rect', g$grobs[[i]]$grobs[[1]]$childrenOrder))
# g$grobs[[i]]$grobs[[1]]$children[[j]]$gp$fill <- fills[k]
# k <- k+1
# }
# grid.draw(g)

#ggsave('figures/hucTrends18panel.png', plot = g, width = 7.25, height = 8, units = 'in', dpi = 300)


## Bring the above together with the map, really just not working. Doin it in illustrator.
# ggdraw() +
#   draw_plot(p, width = 1, height = .9) +
#   draw_plot(lmMap, x = .3, y = .5, width = .5, height = .5)

```

## Look at some correlations

```{r}
#Between watershed stability correlations
stability <- bootstrapped.ts %>%
  group_by(region) %>%
  nest() %>%
  mutate(stability.cor = purrr::map(data, ~cor(.$mean, .$se)),
         time.stab.cor = purrr::map(data, ~cor(.$se, .$year)))

ggplot(bootstrapped.ts, aes(x = year, y = se)) + geom_point() + geom_smooth(method = lm) + facet_wrap(~region, scales = 'free') +
  ggtitle('Trend Stability Over Time')

```


## Take a look at correlations with NADP and PRISM data

```{r, include = F, eval = F}
#Unzip and join all the prism data
## Pull in PRISM data (originally downloaded through the PRISM ftp)
##Everything is zipped, which is super annoying
dezip <- function(path, var){
  files <- list.files(path, pattern = '*.zip', full.names = T)
  purrr::map(files, unzip, exdir = 'D:/PRISM_MeanTemp/unzipped')
  }

#Unzip files to local paths
paths <- list.dirs('D:/PRISM_Precip', recursive = F)
purrr::map(paths, dezip)

paths <- list.dirs('D:/PRISM_MeanTemp', recursive = F)
purrr::map(paths, dezip)

## Reproject and simplify regions to match PRISM raster files for the spatial join
regionNAD83 <- region %>% 
  st_simplify(., dTolerance = 15000) %>%
  st_transform(., 4269)

ggplot(regionNAD83) + geom_sf(aes(fill = region))

geo <- regionNAD83
raster.path <- pathTemps[1]
## Make function for pulling out summary mean values per HUC_2
getMeans <- function(raster.path, geo){
  geo <- as(geo, "Spatial")
  image <- raster(raster.path)
  name <- image@data@names
  yearmonth <- str_split(name, pattern = '_')[[1]][5]
  year = substr(yearmonth,1,4)
  month = substr(yearmonth,5,6)
  means <- extract(image, geo, fun = mean, na.rm = T, sp = T)
  means <- means@data
  colnames(means)[3] <- 'value'
  means$year = year
  means$month = month
  return(means)
}

library(raster)
## It's pretty slow, so lets try to put it all in parrallel
pathTemps <- list.files('D:/PRISM_MeanTemp/unzipped', pattern = '[0-9]{6}_bil.bil$', full.names = T)
pathPrecip <- list.files('D:/PRISM_Precip/unzipped', pattern = '[0-9]{6}_bil.bil$', full.names = T)
plan(multiprocess(workers = availableCores()-4))

prismTemps <- pathTemps %>%
  future_map_dfr(getMeans, geo = regionNAD83, .progress = T)

prismPrecip <- pathPrecip %>%
  future_map_dfr(getMeans, geo = regionNAD83, .progress = T)

plan(sequential)

write_feather(prismTemps, paste0('out/PRISMTemp_',area,'.feather'))
write_feather(prismPrecip, paste0('out/PRISMPrecip_',area,'.feather'))

#raster and furrr randomly don't play well together sometimes.  If the above fails go the slow way.

prismTemps <- pathTemps %>%
  map_dfr(getMeans, geo = regionNAD83)

prismPrecip <- pathPrecip %>%
  map_dfr(getMeans, geo = regionNAD83)

# Detach raster package because it doesn't play nice with dplyr
detach("package:raster", unload = TRUE)
```


```{r, include = F, eval = F}
prismTemps <- read_feather('out/PRISMTemp_eco.feather') %>%
  rename_all(tolower) %>%
  mutate_at(vars(year,month), as.numeric) %>%
  filter(month %in% c(5:9)) %>%  
  group_by(region, year) %>%
  summarise(value = mean(value, na.rm = T))
  

prismPrecip <- read_feather('out/PRISMPrecip_eco.feather') %>%
  rename_all(tolower) %>%
  mutate_at(vars(year,month), as.numeric) %>%
  filter(month %in% c(5:9)) %>%  
  group_by(region, year) %>%
  summarise(value = mean(value, na.rm = T))


## Pull in the NADP Data
nadp.sites <- read.csv('../aquaModel/in/NADP/siteList.csv') %>%
  filter(!is.na(lat)) %>%
  st_as_sf(coords = c('long','lat'), crs = 4326) %>%
  st_transform(.,st_crs(region)) %>%
  st_join(region)
  

#ggplot(nadp.sites) +  geom_sf(data =usa) + geom_sf()

nadp.yearly <- read.csv('../aquaModel/in/NADP/NTN-All-m.csv') %>%
  left_join(nadp.sites, by = 'SiteID') %>%
  rename(year = yr) %>%
  select(-c(Criteria1:Criteria3, fullChemLab, svol, ppt, daysSample, elev)) %>%
  filter(month %in% c(5:9)) %>%
  group_by(region, year) %>%
  dplyr::summarize_if(is.numeric, mean, na.rm = T)
  


##Teleconnection Indexes
tc <- read.csv('../aquaModel/in/IsoPdo.csv') %>%
  mutate(year = as.numeric(substr(YrMonth, 1,4)),
         month = as.numeric(substr(YrMonth, 5,6))) %>%
  filter(month %in% c(5:9)) %>%
  group_by(year) %>%
  summarise_at(vars(PDO, PNA, SOI, NAO), mean)


## Make a master dataset of all the stl, nadp, and prism data.
bootstrapped.full <- bootstrapped.ts %>%
  left_join(nadp.yearly) %>%
  left_join(prismTemps %>% rename(Temp = value)) %>%
  left_join(prismPrecip %>% rename(Precip = value)) %>%
  left_join(tc)

write_feather(bootstrapped.full, paste0('out/bootstrappedFull_',iteration,'.feather'))
```


```{r}
bootstrapped.full <- read_feather(paste0('out/stlFullDataSecchi_',area,'.feather'))

# Just take a look at the data a little  
# p <- stl.full %>%
#   select(region, date, trend_mean, pH, raw_mean) %>%
#   gather(trend_mean:raw_mean, key = 'metric', value = 'value') %>%
#   ggplot(., aes(x = date, y = value, color = metric)) + 
#   geom_line(alpha = .3) +
#   geom_smooth(se = F, span = .3) +
#   facet_wrap(~region, scales = 'free')
# 
# ggplotly(p)

#Look at overall correlation
corr.t <- bootstrapped.full %>%
  select(region, mean, NH4, NO3, SO4, pH, PDO, PNA, NAO, Precip, Temp) %>%
  gather(NH4:Temp, key = 'Metric', value = 'Conc') %>%
  group_by(region, Metric) %>%
  nest() %>%
  mutate(cors = purrr::map(data, ~cor.test(x = .x$mean, y = .x$Conc)),
         cor = purrr::map(cors, 'estimate'),
         cor = purrr::map_dbl(cor, 'cor'),
         p.value = purrr::map_dbl(cors, 'p.value')) %>%
  select(-c(data, cors))%>%
  mutate_at(vars(cor:p.value), round, digits =3) %>%
  mutate(sig = ifelse(p.value < .05, 'yes', NA),
         Metric = factor(Metric, levels = c('Temp', 'Precip', 'NO3', 'NH4', 'SO4', 'pH', 'PNA', 'PDO', 'NAO')),
         signal = 'Trend') %>%
  left_join(region)

corr.t %>%
  filter(Metric %in% c('SO4', 'Temp', 'Precip', 'PDO')) %>%
  st_as_sf() %>%
  st_simplify(., dTolerance = 1000) %>%
  ggplot(.) +  geom_sf(aes(fill = cor, color = sig)) + 
  facet_wrap(~Metric) + 
  scale_fill_gradient2(low = 'red', high = 'blue', mid = 'grey', midpoint = 0) +
  scale_color_manual(values = 'black', na.translate = F) +
  theme(axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks = element_blank(),
        #rect = element_blank(),
        legend.position = 'bottom') +
  labs(fill = 'Correlation\nCoefficient', color = 'P.Value < 0.05',
       title = 'Potential Correlates with Seasonality and Overall Trend') +
  guides(color = guide_legend(label = F))

ggsave(paste0('figures/',mod,'RegionalCorrelates.png'), height = 7, width = 7.5, units = 'in')

## Plotly code for zooming in and looking at the data
# bootstrapped.full %>%
#   select(region, date, raw_mean, remainder_mean, trend_mean, seasonal_mean, NH4, SO4, pH, PDO, Precip, Temp) %>% #, NAO, PNA, NO3,) %>%
#   group_by(region) %>%
#   mutate_at(vars(raw_mean:Temp), scale) %>%
#   ungroup() %>%
#   gather(pH, NH4, SO4, key = 'Metric', value = 'Conc') %>%
#   ggplot(., aes(x = date)) +
#   #geom_line(aes(y = Conc, color = Metric)) +
#   geom_smooth(aes(y = Conc, color = Metric), size = .4, span = .1, se = F) +
#   geom_line(aes(y = trend_mean, linetype = 'Trend'), color = "red", size = 1) +
#   guides(linetype = guide_legend(title = '')) +
#   #geom_smooth(span = .2, se = F) + 
#   facet_wrap(~region, scales = 'free')
```


## Take a look at auto-correlations

```{r, include = F, eval = F}
acf <- stl.boots.mean %>%
  group_by(region) %>%
  nest() %>%
  mutate(acf = purrr::map(data, ~acf(.$raw, lag.max = 360, na.action = na.pass)),
         lag = purrr::map(acf, 'lag'),
         acf = purrr::map(acf, 'acf')) %>%
  select(-data) %>%
  unnest() %>%
  mutate(lag = lag/12 + 1984)

ggplot(acf, aes(x = lag, y = acf)) + geom_col() + facet_wrap(~region, scales = 'free')

## Look at how the different regions are correlated to each other
cor <- stl.boots.mean %>%
  select(region, trend, date) %>%
  spread(region, trend) %>%
  select(-date) %>%
  corrgram(., order = T, upper.panel=panel.conf)


## Look at lagged cross correlations
ccf <- bootstrapped.full %>%
  select(region, trend, seasonal, NH4, NO3, SO4, pH, PDO, ISO, Precip, Temp) %>%
  gather(NH4:Temp, key = 'Metric', value = 'Conc') %>%
  group_by(region, Metric) %>%
  nest() %>%
  mutate(ccf = purrr::map(data, ~ccf(.x$seasonal, .x$Conc, lag.max = 36, na.action = na.pass, plot = F)),
         lag = purrr::map(ccf, 'lag'),
         ccf = purrr::map(ccf, 'acf')) %>%
  select(-data) %>%
  unnest() %>%
  mutate(lag = lag/12)

ggplot(ccf %>% filter(Metric == 'Precip'), aes(x = lag, y = ccf)) + geom_line() + facet_wrap(region~Metric, scales = 'free') + geom_hline(yintercept = -.3, color = 'blue', alpha = .4)

```

## Test with ~469 lakes in HUC 7, Upper Mississippi

```{r, include = F, eval = F}
yearly.nest <- Preds.out %>%
  mutate(date = as.POSIXct(paste0(year,'/',month,'/',01)))

dummy <- expand.grid(year = seq(1984,2018), month = c(1:12), samp = factor(c(100,200,300,400,463)))

samps <- tibble(COMID = sample(unique(yearly.nest$COMID), 100), samp = 100) %>%
  bind_rows(tibble(COMID = sample(unique(yearly.nest$COMID), 200), samp = 200)) %>%
  bind_rows(tibble(COMID = sample(unique(yearly.nest$COMID), 300), samp = 300)) %>%
  bind_rows(tibble(COMID = sample(unique(yearly.nest$COMID), 400), samp = 400)) %>%
  bind_rows(tibble(COMID = unique(yearly.nest$COMID), samp = 463))

yearly.median <- Preds.out %>%
  inner_join(samps %>% mutate(samp = factor(samp))) %>%
  filter(year < 2019) %>%
  group_by(year, month, samp, season) %>%
  dplyr::summarizemedian = median(value, na.rm = T),
            sd = sd(value, na.rm =T),
            count = n()) %>%
  right_join(dummy)%>%
  mutate(date = as.POSIXct(paste0(year,'/',month,'/',01))) %>%
  arrange(samp, date)

ggplot() + 
  geom_line(data = yearly.nest %>% filter(region == '3'), aes(group = COMID, x = date, y = value, color = 'Individual Lake'), alpha = .2) + 
 geom_ribbon(data = yearly.median %>% filter(region == '3'), aes(x = date, ymin = median - .12, ymax = median + .12, fill = '95% Confidence\nInterval'), alpha = .9) +
  geom_line(data = yearly.median %>% filter(region == '3'), aes(x = date, y = median, color = 'Watershed Median')) +
  labs(x = 'Date', y = 'Water Clarity (m)', title = 'Individual Lake Timeseries', color = 'Time Series', fill = '') +
  scale_color_manual(values = c('black', 'red')) +
  scale_fill_manual(values = 'pink') +
  theme_bw()

stl <- yearly.median %>%
  group_by(samp) %>%
  nest() %>%
  mutate(stl = purrr::map(data, ~stlplus(x = .$median, t = .$date, n.p = 12, s.window = 25, sub.labels = c(1:12))),
         date = purrr::map(stl, 'time'),
         stl = purrr::map(stl, 'data')) %>%
  select(-data) %>%
  unnest() %>%
  mutate(year = year(date)) %>%
  group_by(year, samp) %>%
  mutate(seas.max = max(seasonal),
         seas.min = min(seasonal),
         seas.mag = seas.max-seas.min) %>%
  ungroup()

stl %>%
  gather(raw:remainder, key = 'Component', value = 'Value') %>%
  ggplot(., aes(date, Value, color = samp)) +
  geom_line() +
  labs(title = 'Decomposition Comparison by Sample size', color = 'Sample\nSize') +
  facet_wrap(~Component, scales = 'free')

stl %>%
  gather(raw:remainder, key = 'Component', value = 'Value') %>%
  ggplot(.) +
  geom_freqpoly(aes(x = Value, y =stat(density), color = samp)) +
  labs(title = 'Decomposition Comparison by Sample size', color = 'Sample\nSize') +
  facet_wrap(~Component, scales = 'free')

stl %>%
  gather(raw:remainder, key = 'Component', value = 'Value') %>%
  ggplot(.) +
  geom_violin(aes(x = samp, y = Value), draw_quantiles = c(.25,.5,.75)) +
  labs(title = 'Decomposition Comparison by Sample size', x = 'Sample Size') +
  facet_wrap(~Component, scales = 'free')

```

## Cluster analysis
```{r, include = F, eval = F}
library(dtwclust)

##Check how many lakes we have lots of obs for
counts <- Preds.out %>% 
  group_by(COMID, year, month) %>%
  summarise(count = n()) %>%
  group_by(COMID) %>%
  summarise(count = n())


t.matrix <- huc2.full %>% select(region, date, trend_mean) %>% filter(!is.na(trend_mean)) %>% spread(key = date, value = trend_mean) %>% as.data.frame() %>% set_rownames(.,levels(huc2.full$region)) %>% select(-region) %>% t() %>% na.omit() %>% t()

pc_t <- tsclust(t.matrix, k = 3,
distance = "dtw_basic", centroid = "mean",
seed = 94L)
cvi(pc_t, type = 'internal')

s.matrix <- huc2.full %>% select(region, date, seasonal_mean) %>% filter(!is.na(seasonal_mean)) %>% spread(key = date, value = seasonal_mean) %>% as.data.frame() %>% set_rownames(.,levels(huc2.full$region)) %>% select(-region) %>% t() %>% na.omit() %>% t()

pc_s <- tsclust(s.matrix, k = 3,
distance = "dtw_basic", centroid = "mean",
seed = 94L)
cvi(pc_s, type = 'internal')

r.matrix <- huc2.full %>% select(region, date, raw_mean) %>% filter(!is.na(raw_mean)) %>% spread(key = date, value = raw_mean) %>% as.data.frame() %>% set_rownames(.,levels(huc2.full$region)) %>% select(-region) %>% t() %>% na.omit() %>% t()

pc_r <- tsclust(r.matrix, k = 3,
distance = "dtw_basic", centroid = "mean",
seed = 94L)
cvi(pc_r, type = 'internal')

clusters <- tibble(region = row.names(t.matrix), tClust = pc_t@cluster, sClust = pc_s@cluster, rClust = pc_r@cluster)

check <- clusters %>%
  gather(tClust:rClust, key = 'Component', value = 'Cluster') %>%
  left_join(region)


p <- ggplot(check %>% mutate(Cluster = factor(Cluster))) +
  geom_sf(aes(fill = Cluster)) + 
  theme_bw() +
  theme(axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks = element_blank(),
        #rect = element_blank(),
        legend.position = 'bottom') +
  facet_wrap(~Component)

#ggsave(p, file = 'figures/ClusterAnalysis.png', width = 7, height = 4, units = 'in')


huc2.full <- huc2.full %>% left_join(clusters)

p1 <- huc2.full %>%
  ggplot(aes(x = date, y = trend_mean, color = region, group = region)) + 
  geom_ribbon(aes(ymin = trend_mean - trend_sd, ymax = trend_mean + trend_sd), fill = 'grey90', color = 'grey90') +
  geom_line() + 
  scale_color_viridis_d() +
  theme_bw() +
  theme(legend.position = 'none') +
  facet_wrap(~tClust, nrow = 2, scales = 'free')

p1

grid.arrange(grobs = list(clustTrends, hucSamples), nrow = 2)

## Code for comparing n clusters
# pc_k <- tsclust(s.matrix, k = 3:6,
# distance = "dtw_basic", centroid = "mean",
# seed = 94L)
# names(pc_k) <- paste0("k_", 3L:6L)
# sapply(pc_k, cvi, type = "internal")
```

## PCA to look at similarities
```{r, include = F, eval = F}
library(factoextra)
library(FactoMineR)

t.pca <- huc2.full %>% 
  select(region, date, trend_mean) %>%
  mutate(year = year(date)) %>%
  group_by(year, region) %>%
  dplyr::summarize(trend = mean(trend_mean, na.rm = T)) %>%
  group_by(region) %>%
  mutate(trend = scale(trend)) %>%
  ungroup() %>%
  spread(key = year, value = trend) %>%
  #arrange(year) %>%
  column_to_rownames(.,'region')#%>%
  #as.data.frame() %>%
  #select(-year) %>%
  svd()

write.csv(t.pca, 'out/TedData.csv')
fviz_eig(t.pca)

fviz_pca_var(t.pca,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )

fviz_pca_ind(t.pca,
             col.ind = "cos2", # Color by the quality of representation
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )

fviz_pca_biplot(t.pca, repel = TRUE,
                col.var = "#2E9FDF", # Variables color
                col.ind = "#696969"  # Individuals color
                )


t.pca$var$coord

```

NLA Comparison

```{r}
nla.2007 <- read.csv('in/NLA/nla2007_secchi_20091008.txt', stringsAsFactors = F) %>%
  select(SITE_ID, secchi = SECMEAN, date = DATE_SECCHI) %>%
  left_join(read.csv('in/NLA/nla2007_sampledlakeinformation_20091113.txt') %>%
              select(SITE_ID, COMID = COM_ID, region = WSA_ECO9, 
                     lat = LAT_DD, long = LON_DD))

nla.2012 <- read.csv('in/NLA/nla2012_secchi_08232016.txt', stringsAsFactors = F) %>%
  select(SITE_ID, secchi = SECCHI, date = DATE_COL) %>%
  left_join(read.csv('in/NLA/nla2012_wide_siteinfo_08232016.txt') %>%
              select(SITE_ID, COMID = COMID2012, region = AGGR_ECO9_2015, 
                     lat = LAT_DD83, long = LON_DD83))

nlaFull <- nla.2007 %>%
  mutate(nla.year = 2007) %>%
  bind_rows(nla.2012 %>% mutate(nla.year = 2012)) %>%
  mutate(date = mdy(date),
         year = year(date),
         month = month(date),
         region = factor(region, 
                         labels = c("Coastal Plain", "Northern Appalachians", "Northern Plains", "Southern Appalachians", "Southern Plains", "Temperate Planes", "Upper Midwest","Western Mountains","Xeric West"))) %>%
  group_by(SITE_ID, region, nla.year) %>%
  summarise(secchi = median(secchi, na.rm = T)) %>%
  group_by(region, nla.year) %>%
  rename(year = nla.year) %>%
  summarise(secchi.mean = mean(secchi, na.rm = T),
            secchi.median = median(secchi, na.rm = T),
            secchi.sd = sd(secchi, na.rm =T)) %>%
  mutate(source = 'NLA')

predComp <- Preds.out %>% filter(year %in% c(2007,2012),
                                 month %in% c(5:9)) %>%
  group_by(region, year) %>%
  summarise(secchi.mean = mean(value, na.rm = T),
            secchi.median = median(value, na.rm = T),
            secchi.sd = sd(value, na.rm =T)) %>%
  mutate(source = 'RS') %>%
  bind_rows(nlaFull)

ggplot(predComp, aes(x = region, color = source)) +
  geom_point(aes(y = secchi.mean)) +
  geom_errorbar(aes(ymin = secchi.mean - secchi.sd, ymax = secchi.mean + secchi.sd)) +
  scale_color_viridis_d(end = .6) +
  facet_wrap(~year) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(x = 'Region', y = 'Water Clarity (m)', title = 'Comparison of NLA to Remotely Sensed Predictions')


predComp <- Preds.out %>% filter(year %in% c(2007,2012),
                                 month %in% c(5:9)) %>%
  select(region, secchi = value, year) %>%
  mutate(source = 'RS') %>%
  bind_rows(nla.2007 %>% select(region, secchi) %>%
              mutate(source = 'NLA', year = 2007)) %>%
  bind_rows(nla.2012 %>% select(region, secchi) %>%
              mutate(source = 'NLA', year = 2012))

ggplot(predComp, aes(x = region, y = secchi, color = source)) + 
  geom_violin()  + 
  scale_color_viridis_d(end = .6) +
  scale_y_continuous(limits = c(0,10)) +
  facet_wrap(~year, ncol = 2) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
        
        
```



# Quick look at seasonality

```{r}
seasonal <- Preds.out %>%
  mutate(year.int = cut_interval(year, 10)) %>%
  filter(month %in% c(4:10)) %>%
  group_by(region, month, year.int) %>%
  summarise(value = mean(value))

ggplot(seasonal, aes(x = month, y = value, color = year.int)) + geom_line() + facet_wrap(~region, scales = 'free') + scale_color_viridis_d()
```

